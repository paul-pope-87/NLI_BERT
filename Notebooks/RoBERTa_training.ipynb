{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddc7e02",
   "metadata": {},
   "source": [
    "<h1> Second Round of BERT Training </h1>\n",
    "\n",
    "In this round, I follow the RoBERTa implementation, which uses dynamic masking and whole-text prediction (rather than NSP). Named entity tokens in the dataset have now been replaced with their BIOES tags. I also add regularization in attempt to balance the distribution of samples across targets in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "97c8c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pyto_env kernel, pyto env.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import spacy \n",
    "import importlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adagrad, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, PretrainedConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "from transformers import (Trainer, \n",
    "                          RobertaTokenizer, \n",
    "                          RobertaModel, \n",
    "                          RobertaForSequenceClassification, \n",
    "                          RobertaConfig, \n",
    "                          RobertaForMaskedLM)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, default_data_collator\n",
    "\n",
    "import common\n",
    "from common import (ClassificationDataset, \n",
    "                    BERT_fine_tune_validation, \n",
    "                    BERT_fine_tune_train, \n",
    "                    plot_confusion_matrix, \n",
    "                    train_test_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7542b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/Users/paulp/Library/CloudStorage/OneDrive-UniversityofEasternFinland/UEF/Thesis\"\n",
    "data_dir = os.path.join(project_dir,\"Data\")\n",
    "model_dir = os.path.join(project_dir, \"Models\")\n",
    "roberta_dir = os.path.join(model_dir, 'robert-classifier')\n",
    "\n",
    "os.chdir(data_dir)\n",
    "\n",
    "masked_dataset = pd.read_csv('masked_data_set.csv', index_col = 0)\n",
    "\n",
    "#L1 to integer map for loading categories into BERT\n",
    "with open('target_idx.json') as f:\n",
    "    data = f.read()\n",
    "target_idx = json.loads(data)\n",
    "idx_target = {target_idx[a]:a for a in target_idx.keys()}\n",
    "\n",
    "n_classes = len(target_idx.keys())\n",
    "\n",
    "# additional special tokens\n",
    "with open('spec_tokens_ne.txt', 'rb') as file:\n",
    "    spec_tokens = pickle.load(file)\n",
    "    \n",
    "# Load from data directory\n",
    "masked_dataset = pd.read_csv('masked_data_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a02fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908860a",
   "metadata": {},
   "source": [
    "<h1> RoBERTa </h1>\n",
    "\n",
    "<h2> Create Pre-training Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b8c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def remove_blanklines(text):\n",
    "    no_newline = re.sub('\\n\\n', '\\n\\t', text)\n",
    "    return no_newline\n",
    "\n",
    "new_dataset = masked_dataset\n",
    "new_dataset['Text'] = masked_dataset['Text'].apply(lambda x : remove_blanklines(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1134f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_dataset(df, sampler, n):\n",
    "    ds = ''\n",
    "    for a in range(n):\n",
    "        text = df['Text'].iloc[next(sampler)]\n",
    "        ds = ds + '\\n\\n\\n\\n' + text\n",
    "    return ds\n",
    "\n",
    "def tr_ts_vl_split(df, tr_size=0.85, vl_size=0.075):\n",
    "    \n",
    "    sampler = RandomSampler(df)\n",
    "    iterator = iter(sampler)\n",
    "    \n",
    "    n_samples = len(sampler)\n",
    "    ts_size = 1.0-tr_size-vl_size\n",
    "    \n",
    "    train_size = round(tr_size*n_samples)\n",
    "    val_size = round(vl_size*n_samples)\n",
    "    test_size = n_samples - train_size - val_size\n",
    "    \n",
    "    train_ds = sample_dataset(df, iterator, train_size)\n",
    "    val_ds = sample_dataset(df, iterator, val_size)\n",
    "    test_ds = sample_dataset(df, iterator, test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29633538",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = tr_ts_vl_split(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c40eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('roberta_pretrain_train_ds.txt', 'w') as file:\n",
    "    file.write(train_ds)\n",
    "with open('roberta_pretrain_val_ds.txt', 'w') as file:\n",
    "    file.write(val_ds)\n",
    "with open('roberta_pretain_test_ds.txt', 'w') as file:\n",
    "    file.write(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007a2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d479251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99536b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.examples.roberta import multiprocessing_bpe_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0e84d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package fairseq:\n",
      "\n",
      "NAME\n",
      "    fairseq - isort:skip_file\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    benchmark (package)\n",
      "    binarizer\n",
      "    checkpoint_utils\n",
      "    config (package)\n",
      "    criterions (package)\n",
      "    data (package)\n",
      "    dataclass (package)\n",
      "    distributed (package)\n",
      "    examples (package)\n",
      "    file_chunker_utils\n",
      "    file_io\n",
      "    file_utils\n",
      "    hub_utils\n",
      "    incremental_decoding_utils\n",
      "    iterative_refinement_generator\n",
      "    libbleu\n",
      "    logging (package)\n",
      "    model_parallel (package)\n",
      "    models (package)\n",
      "    modules (package)\n",
      "    nan_detector\n",
      "    ngram_repeat_block\n",
      "    optim (package)\n",
      "    options\n",
      "    pdb\n",
      "    quantization_utils\n",
      "    registry\n",
      "    scoring (package)\n",
      "    search\n",
      "    sequence_generator\n",
      "    sequence_scorer\n",
      "    speech_generator\n",
      "    tasks (package)\n",
      "    token_generation_constraints\n",
      "    tokenizer\n",
      "    trainer\n",
      "    utils\n",
      "    version\n",
      "\n",
      "SUBMODULES\n",
      "    distributed_utils\n",
      "    meters\n",
      "    metrics\n",
      "    progress_bar\n",
      "\n",
      "DATA\n",
      "    __all__ = ['pdb']\n",
      "\n",
      "VERSION\n",
      "    0.12.2\n",
      "\n",
      "FILE\n",
      "    /Users/paulp/miniforge3/envs/pyto/lib/python3.9/site-packages/fairseq/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fairseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b85130",
   "metadata": {},
   "source": [
    "The following is adapted from the fairseq.examples.roberta, and runs from the shell but not with Jupyter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938882e",
   "metadata": {},
   "source": [
    "#cd ../Data\n",
    "#mkdir -p gpt2_bpe\n",
    "\n",
    "#wget -O --no-check-certificate gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "#wget -O --no-check-certificate gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe \\\n",
    "\n",
    "for SPLIT in train val test \n",
    "    do\n",
    "        python -m fairseq.examples.roberta.multiprocessing_bpe_encoder \\\n",
    "            --encoder-json gpt2_bpe/encoder.json \\\n",
    "            --vocab-bpe gpt2_bpe/vocab.bpe \\\n",
    "            --inputs roberta_pretrain_${SPLIT}_ds.txt \\\n",
    "            --outputs roberta_pretrain_${SPLIT}.bpe \\\n",
    "            --keep-empty \\\n",
    "            --workers 60; \\\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b386b58",
   "metadata": {},
   "source": [
    "mkdir ./roberta_pretrain_processed\n",
    "\n",
    "wget --no-check-certificate -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict gpt2_bpe/dict.txt \\\n",
    "    --trainpref roberta_pretrain_train.bpe \\\n",
    "    --validpref roberta_pretrain_val.bpe \\\n",
    "    --testpref roberta_pretrain_test.bpe \\\n",
    "    --destdir roberta_pretrain_processed \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dea7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19791551",
   "metadata": {},
   "source": [
    "<h2> Pre-training </h2>\n",
    "\n",
    "Skip for now. Only fine-tune from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaca95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "from fairseq import distributed_utils, metrics\n",
    "from fairseq.dataclass.configs import FairseqConfig\n",
    "from fairseq.dataclass.initialize import add_defaults, hydra_init\n",
    "from fairseq.dataclass.utils import omegaconf_no_object_check\n",
    "from fairseq.utils import reset_logging\n",
    "from fairseq_cli.train import main as pre_main\n",
    "\n",
    "logger = logging.getLogger(\"fairseq_cli.hydra_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935756a",
   "metadata": {},
   "source": [
    "DATA_DIR=roberta_pretrain_processed\n",
    "\n",
    "fairseq-hydra-train -m --config-dir . \\\n",
    "--config-name roberta_pretrain_config \\\n",
    "task.data=$DATA_DIR \\\n",
    "checkpoint.restore_file= ./roberta_for_pretraining_256.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738ea24",
   "metadata": {},
   "source": [
    "<h1> Fine-Tuning </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ca7be5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_tr, ds_vl, ds_ts = train_test_val_split(masked_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41251d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', \n",
    "                                             #additional_special_tokens = spec_tokens, #this doesn't work with vocab_size \n",
    "                                            )\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb52b7",
   "metadata": {},
   "source": [
    "<h2> Weighted Cross-Entropy Loss </h2>"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "caa4ad86",
   "metadata": {},
   "source": [
    "Swap the loss function in RoBERTa: code below adapted from:\n",
    "https://github.com/huggingface/transformers/issues/10845\n",
    "\n",
    "Weighted Cross-Entropy:\n",
    "https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab\n",
    "![image.png](attachment:image.png)\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "6a862991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "beta = 0.999\n",
    "samples_per_class = ds_tr['Target'].value_counts()\n",
    "effective_num = 1.0 - np.power(beta, samples_per_class)\n",
    "weights = (1.0 - beta) / effective_num\n",
    "weights = weights / np.sum(weights)*n_classes\n",
    "# these weights were too heavy\n",
    "#w = ds_tr['Target'].value_counts().transform(lambda x: 1/x)*1000\n",
    "# IMPORTANT: connect the weights to the target IDS\n",
    "weights = torch.tensor([w[a] for a in target_idx]).to(torch.float32)\n",
    "#weights = torch.tensor(weights + np.mean(weights))\n",
    "#focal_loss = FocalLoss(gamma = 2.0,\n",
    "#                      weights = weights,\n",
    "#                      reduction = 'mean',\n",
    "#                      eps = 1e-5,\n",
    "#                      ignore_index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "525f6982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2551, 0.4136, 0.6238, 0.5917, 0.3720, 0.6689])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "71f23584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                                    num_labels = n_classes,\n",
    "                                                                     attention_probs_dropout_prob = 0.05,\n",
    "                                                                     hidden_dropout_prob = 0.05,\n",
    "                                                                    classifier_dropout = 0.0,\n",
    "                                                                    id2label = idx_target,\n",
    "                                                                    label2id = target_idx,\n",
    "                                                                output_attentions = True,\n",
    "                                                                output_hidden_states = True)\n",
    "\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-cased', \n",
    "                                                          num_labels = n_classes,\n",
    "                                                          attention_probs_dropout_prob = 0.05,\n",
    "                                                          hidden_dropout_prob = 0.05,\n",
    "                                                          classifier_dropout = 0.0,\n",
    "                                                          id2label = idx_target,\n",
    "                                                          label2id = target_idx,\n",
    "                                                          output_attentions = True,\n",
    "                                                          output_hidden_states = True)\n",
    "\n",
    "bert_base = BertForMaskedLM.from_pretrained('bert-base-cased',\n",
    "                                            output_attentions = True,\n",
    "                                            output_hidden_states = True)\n",
    "\n",
    "roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base',\n",
    "                                                  output_attentions = True,\n",
    "                                                  output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tokens to model vocabulary:\n",
    "bert_vocab = bert_model.get_vocab() \n",
    "roberta_vocab = roberta_model.get_vocab()\n",
    "\n",
    "for j,k in spec_tokens:\n",
    "    \n",
    "bert_vocab.update({'mynewword' : len(bert_vocab)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "690fe848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, num_labels, loss_fn): #checkpoint, num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = model# = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.00) \n",
    "        self.linear = nn.Linear(512, 1)\n",
    "        self.layer_norm = nn.LayerNorm([768, 1])\n",
    "        self.classifier = nn.Linear(768, num_labels) # load and initialize weights\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids = None, attention_mask=None,labels=None):\n",
    "        #Extract outputs from the MLM model\n",
    "        outputs = self.model(input_ids=input_ids, token_type_ids = token_type_ids, attention_mask=attention_mask)\n",
    "        #Get Last hidden state\n",
    "        sequence_output = self.dropout(outputs['hidden_states'][-1])\n",
    "        # condense the 512 tokens into 1 dimension\n",
    "        token_crunch = self.linear(sequence_output.transpose(-2,-1)) \n",
    "        normed = self.layer_norm(token_crunch)\n",
    "        # condense the features into 6 class values \n",
    "        logits = self.classifier(normed.transpose(-2,-1)).squeeze(1)\n",
    "        \n",
    "        # calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          #loss_fct = nn.CrossEntropyLoss(weight = class_weights)\n",
    "            loss = self.loss_fn(logits, labels.long())\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, \n",
    "                                        logits=logits, \n",
    "                                        hidden_states=outputs.hidden_states, \n",
    "                                        attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "c2c10cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_cross_entropy_loss = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "roberta_custom_wgt_cel = CustomModel(roberta_base,\n",
    "                                    num_labels = n_classes,\n",
    "                                    loss_fn = weighted_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d53fd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l = nn.Linear(in_features = 512, out_features = 1)\n",
    "d = nn.Dropout(0.1)\n",
    "c = nn.Linear(in_features = 768, out_features = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0cf644e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "last_hidden = o['hidden_states'][-1]\n",
    "print(last_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "39133b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 512])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "d90708da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 1])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " m = l(last_hidden.transpose(-2,-1))\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4e699486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "n = d(m)\n",
    "print(n.shape, n.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "05bc7c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6]) torch.float32 tensor([[ 0.1494, -0.0362,  0.0212,  0.0374,  0.0029,  0.0714],\n",
      "        [ 0.1220, -0.0217,  0.0216, -0.0450,  0.1950,  0.0048]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "j = c(n.transpose(-2,-1)).squeeze(1).float()\n",
    "print(j.shape, j.dtype, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5fe4f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.Size([2]) tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(labels.dtype, labels.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c6fc568c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2551, 0.4136, 0.6238, 0.5917, 0.3720, 0.6689])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "bafc2df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7953, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = nn.CrossEntropyLoss(weights)\n",
    "h(j, labels)#.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "86c2b4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1212, -0.0323,  0.0100,  0.0713, -0.0189,  0.0911],\n",
       "        [ 0.1390, -0.0163,  0.0406, -0.0970,  0.2217, -0.0293]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e2590625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2551, 0.4136, 0.6238, 0.5917, 0.3720, 0.6689], dtype=torch.float64)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b425e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1494, -0.0362,  0.0212,  0.0374,  0.0029,  0.0714],\n",
      "        [ 0.1220, -0.0217,  0.0216, -0.0450,  0.1950,  0.0048]],\n",
      "       grad_fn=<SqueezeBackward0>) tensor([0, 1]) tensor(1.7953, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(weight=weights)\n",
    "inpt = j.squeeze()#torch.randn(3, 6, requires_grad=True)\n",
    "target = labels.long()#torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(inpt, target)\n",
    "#output.backward()\n",
    "print(inpt, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0a027ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CrossEntropyLoss in module torch.nn.modules.loss:\n",
      "\n",
      "class CrossEntropyLoss(_WeightedLoss)\n",
      " |  CrossEntropyLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0) -> None\n",
      " |  \n",
      " |  This criterion computes the cross entropy loss between input and target.\n",
      " |  \n",
      " |  It is useful when training a classification problem with `C` classes.\n",
      " |  If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
      " |  assigning weight to each of the classes.\n",
      " |  This is particularly useful when you have an unbalanced training set.\n",
      " |  \n",
      " |  The `input` is expected to contain raw, unnormalized scores for each class.\n",
      " |  `input` has to be a Tensor of size :math:`(C)` for unbatched input,\n",
      " |  :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n",
      " |  `K`-dimensional case. The last being useful for higher dimension inputs, such\n",
      " |  as computing cross entropy loss per-pixel for 2D images.\n",
      " |  \n",
      " |  The `target` that this criterion expects should contain either:\n",
      " |  \n",
      " |  - Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n",
      " |    `ignore_index` is specified, this loss also accepts this class index (this index\n",
      " |    may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n",
      " |    set to ``'none'``) loss for this case can be described as:\n",
      " |  \n",
      " |    .. math::\n",
      " |        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      " |        l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
      " |        \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
      " |  \n",
      " |    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
      " |    :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
      " |    :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
      " |    :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
      " |  \n",
      " |    .. math::\n",
      " |        \\ell(x, y) = \\begin{cases}\n",
      " |            \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n",
      " |             \\text{if reduction} = \\text{`mean';}\\\\\n",
      " |              \\sum_{n=1}^N l_n,  &\n",
      " |              \\text{if reduction} = \\text{`sum'.}\n",
      " |          \\end{cases}\n",
      " |  \n",
      " |    Note that this case is equivalent to the combination of :class:`~torch.nn.LogSoftmax` and\n",
      " |    :class:`~torch.nn.NLLLoss`.\n",
      " |  \n",
      " |  - Probabilities for each class; useful when labels beyond a single class per minibatch item\n",
      " |    are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n",
      " |    :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n",
      " |  \n",
      " |    .. math::\n",
      " |        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      " |        l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n",
      " |  \n",
      " |    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
      " |    :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
      " |    :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
      " |    :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
      " |  \n",
      " |    .. math::\n",
      " |        \\ell(x, y) = \\begin{cases}\n",
      " |            \\frac{\\sum_{n=1}^N l_n}{N}, &\n",
      " |             \\text{if reduction} = \\text{`mean';}\\\\\n",
      " |              \\sum_{n=1}^N l_n,  &\n",
      " |              \\text{if reduction} = \\text{`sum'.}\n",
      " |          \\end{cases}\n",
      " |  \n",
      " |  .. note::\n",
      " |      The performance of this criterion is generally better when `target` contains class\n",
      " |      indices, as this allows for optimized computation. Consider providing `target` as\n",
      " |      class probabilities only when a single class label per minibatch item is too restrictive.\n",
      " |  \n",
      " |  Args:\n",
      " |      weight (Tensor, optional): a manual rescaling weight given to each class.\n",
      " |          If given, has to be a Tensor of size `C`\n",
      " |      size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      " |          the losses are averaged over each loss element in the batch. Note that for\n",
      " |          some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
      " |          is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      " |          when :attr:`reduce` is ``False``. Default: ``True``\n",
      " |      ignore_index (int, optional): Specifies a target value that is ignored\n",
      " |          and does not contribute to the input gradient. When :attr:`size_average` is\n",
      " |          ``True``, the loss is averaged over non-ignored targets. Note that\n",
      " |          :attr:`ignore_index` is only applicable when the target contains class indices.\n",
      " |      reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      " |          losses are averaged or summed over observations for each minibatch depending\n",
      " |          on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      " |          batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      " |      reduction (string, optional): Specifies the reduction to apply to the output:\n",
      " |          ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n",
      " |          be applied, ``'mean'``: the weighted mean of the output is taken,\n",
      " |          ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      " |          and :attr:`reduce` are in the process of being deprecated, and in\n",
      " |          the meantime, specifying either of those two args will override\n",
      " |          :attr:`reduction`. Default: ``'mean'``\n",
      " |      label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
      " |          of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
      " |          become a mixture of the original ground truth and a uniform distribution as described in\n",
      " |          `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      " |        in the case of `K`-dimensional loss.\n",
      " |      - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
      " |        :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
      " |        If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
      " |      - Output: If reduction is 'none', same shape as the target. Otherwise, scalar.\n",
      " |  \n",
      " |      where:\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              C ={} & \\text{number of classes} \\\\\n",
      " |              N ={} & \\text{batch size} \\\\\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> # Example of target with class indices\n",
      " |      >>> loss = nn.CrossEntropyLoss()\n",
      " |      >>> input = torch.randn(3, 5, requires_grad=True)\n",
      " |      >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
      " |      >>> output = loss(input, target)\n",
      " |      >>> output.backward()\n",
      " |      >>>\n",
      " |      >>> # Example of target with class probabilities\n",
      " |      >>> input = torch.randn(3, 5, requires_grad=True)\n",
      " |      >>> target = torch.randn(3, 5).softmax(dim=1)\n",
      " |      >>> output = loss(input, target)\n",
      " |      >>> output.backward()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CrossEntropyLoss\n",
      " |      _WeightedLoss\n",
      " |      _Loss\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0) -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'ignore_index': <class 'int'>, 'label_smoothing': <...\n",
      " |  \n",
      " |  __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearning out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "1ae69280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CustomModel(\n",
       "  (model): RobertaForMaskedLM(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_custom_wgt_cel.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "59f3532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(roberta_custom_wgt_cel.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "b31fb645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    50 / 6,907  -  Avg Batch Loss: 1.9658  -  Elapsed: 0:01:59\n",
      "  Batch   100 / 6,907  -  Avg Batch Loss: 1.8570  -  Elapsed: 0:03:56\n",
      "  Batch   150 / 6,907  -  Avg Batch Loss: 1.8801  -  Elapsed: 0:05:52\n",
      "  Batch   200 / 6,907  -  Avg Batch Loss: 1.8998  -  Elapsed: 0:07:49\n",
      "  Batch   250 / 6,907  -  Avg Batch Loss: 1.7952  -  Elapsed: 0:09:44\n",
      "  Batch   300 / 6,907  -  Avg Batch Loss: 1.8556  -  Elapsed: 0:11:40\n",
      "  Batch   350 / 6,907  -  Avg Batch Loss: 1.8890  -  Elapsed: 0:13:34\n",
      "  Batch   400 / 6,907  -  Avg Batch Loss: 1.8623  -  Elapsed: 0:15:28\n",
      "  Batch   450 / 6,907  -  Avg Batch Loss: 1.8118  -  Elapsed: 0:17:22\n",
      "  Batch   500 / 6,907  -  Avg Batch Loss: 1.7937  -  Elapsed: 0:19:18\n",
      "  Batch   550 / 6,907  -  Avg Batch Loss: 1.8410  -  Elapsed: 0:21:16\n",
      "  Batch   600 / 6,907  -  Avg Batch Loss: 1.8522  -  Elapsed: 0:23:18\n",
      "  Batch   650 / 6,907  -  Avg Batch Loss: 1.8506  -  Elapsed: 0:25:18\n",
      "  Batch   700 / 6,907  -  Avg Batch Loss: 1.7902  -  Elapsed: 0:27:20\n",
      "  Batch   750 / 6,907  -  Avg Batch Loss: 1.8720  -  Elapsed: 0:29:21\n",
      "  Batch   800 / 6,907  -  Avg Batch Loss: 1.8521  -  Elapsed: 0:31:22\n",
      "  Batch   850 / 6,907  -  Avg Batch Loss: 1.8009  -  Elapsed: 0:33:23\n",
      "  Batch   900 / 6,907  -  Avg Batch Loss: 1.8083  -  Elapsed: 0:35:23\n",
      "  Batch   950 / 6,907  -  Avg Batch Loss: 1.8227  -  Elapsed: 0:37:24\n",
      "  Batch 1,000 / 6,907  -  Avg Batch Loss: 1.8079  -  Elapsed: 0:39:25\n",
      "  Batch 1,050 / 6,907  -  Avg Batch Loss: 1.8571  -  Elapsed: 0:41:27\n",
      "  Batch 1,100 / 6,907  -  Avg Batch Loss: 1.8830  -  Elapsed: 0:43:29\n",
      "  Batch 1,150 / 6,907  -  Avg Batch Loss: 1.8353  -  Elapsed: 0:45:31\n",
      "  Batch 1,200 / 6,907  -  Avg Batch Loss: 1.8347  -  Elapsed: 0:47:33\n",
      "  Batch 1,250 / 6,907  -  Avg Batch Loss: 1.8413  -  Elapsed: 0:49:36\n",
      "  Batch 1,300 / 6,907  -  Avg Batch Loss: 1.7844  -  Elapsed: 0:51:40\n",
      "  Batch 1,350 / 6,907  -  Avg Batch Loss: 1.7944  -  Elapsed: 0:53:43\n",
      "  Batch 1,400 / 6,907  -  Avg Batch Loss: 1.7893  -  Elapsed: 0:55:45\n",
      "  Batch 1,450 / 6,907  -  Avg Batch Loss: 1.7621  -  Elapsed: 0:57:47\n",
      "  Batch 1,500 / 6,907  -  Avg Batch Loss: 1.8556  -  Elapsed: 0:59:49\n",
      "  Batch 1,550 / 6,907  -  Avg Batch Loss: 1.8535  -  Elapsed: 1:01:48\n",
      "  Batch 1,600 / 6,907  -  Avg Batch Loss: 1.8324  -  Elapsed: 1:03:41\n",
      "  Batch 1,650 / 6,907  -  Avg Batch Loss: 1.7426  -  Elapsed: 1:05:34\n",
      "  Batch 1,700 / 6,907  -  Avg Batch Loss: 1.8632  -  Elapsed: 1:07:24\n",
      "  Batch 1,750 / 6,907  -  Avg Batch Loss: 1.8302  -  Elapsed: 1:09:14\n",
      "  Batch 1,800 / 6,907  -  Avg Batch Loss: 1.8630  -  Elapsed: 1:11:04\n",
      "  Batch 1,850 / 6,907  -  Avg Batch Loss: 1.7621  -  Elapsed: 1:12:53\n",
      "  Batch 1,900 / 6,907  -  Avg Batch Loss: 1.8236  -  Elapsed: 1:14:43\n",
      "  Batch 1,950 / 6,907  -  Avg Batch Loss: 1.7545  -  Elapsed: 1:16:35\n",
      "  Batch 2,000 / 6,907  -  Avg Batch Loss: 1.7491  -  Elapsed: 1:18:26\n",
      "  Batch 2,050 / 6,907  -  Avg Batch Loss: 1.8439  -  Elapsed: 1:20:17\n",
      "  Batch 2,100 / 6,907  -  Avg Batch Loss: 1.7917  -  Elapsed: 1:22:10\n",
      "  Batch 2,150 / 6,907  -  Avg Batch Loss: 1.7728  -  Elapsed: 1:24:01\n",
      "  Batch 2,200 / 6,907  -  Avg Batch Loss: 1.7653  -  Elapsed: 1:25:52\n",
      "  Batch 2,250 / 6,907  -  Avg Batch Loss: 1.7930  -  Elapsed: 1:27:46\n",
      "  Batch 2,300 / 6,907  -  Avg Batch Loss: 1.8266  -  Elapsed: 1:29:43\n",
      "  Batch 2,350 / 6,907  -  Avg Batch Loss: 1.8488  -  Elapsed: 1:31:39\n",
      "  Batch 2,400 / 6,907  -  Avg Batch Loss: 1.7935  -  Elapsed: 1:33:34\n",
      "  Batch 2,450 / 6,907  -  Avg Batch Loss: 1.8282  -  Elapsed: 1:35:28\n",
      "  Batch 2,500 / 6,907  -  Avg Batch Loss: 1.8204  -  Elapsed: 1:37:22\n",
      "  Batch 2,550 / 6,907  -  Avg Batch Loss: 1.7705  -  Elapsed: 1:39:18\n",
      "  Batch 2,600 / 6,907  -  Avg Batch Loss: 1.7566  -  Elapsed: 1:41:13\n",
      "  Batch 2,650 / 6,907  -  Avg Batch Loss: 1.8758  -  Elapsed: 1:43:08\n",
      "  Batch 2,700 / 6,907  -  Avg Batch Loss: 1.8208  -  Elapsed: 1:45:02\n",
      "  Batch 2,750 / 6,907  -  Avg Batch Loss: 1.8162  -  Elapsed: 1:46:57\n",
      "  Batch 2,800 / 6,907  -  Avg Batch Loss: 1.8199  -  Elapsed: 1:48:53\n",
      "  Batch 2,850 / 6,907  -  Avg Batch Loss: 1.8169  -  Elapsed: 1:50:48\n",
      "  Batch 2,900 / 6,907  -  Avg Batch Loss: 1.8254  -  Elapsed: 1:52:41\n",
      "  Batch 2,950 / 6,907  -  Avg Batch Loss: 1.7900  -  Elapsed: 1:54:32\n",
      "  Batch 3,000 / 6,907  -  Avg Batch Loss: 1.8395  -  Elapsed: 1:56:30\n",
      "  Batch 3,050 / 6,907  -  Avg Batch Loss: 1.7455  -  Elapsed: 1:58:23\n",
      "  Batch 3,100 / 6,907  -  Avg Batch Loss: 1.8016  -  Elapsed: 2:00:17\n",
      "  Batch 3,150 / 6,907  -  Avg Batch Loss: 1.8065  -  Elapsed: 2:02:12\n",
      "  Batch 3,200 / 6,907  -  Avg Batch Loss: 1.7541  -  Elapsed: 2:04:07\n",
      "  Batch 3,250 / 6,907  -  Avg Batch Loss: 1.8459  -  Elapsed: 2:06:01\n",
      "  Batch 3,300 / 6,907  -  Avg Batch Loss: 1.7463  -  Elapsed: 2:07:57\n",
      "  Batch 3,350 / 6,907  -  Avg Batch Loss: 1.8228  -  Elapsed: 2:09:51\n",
      "  Batch 3,400 / 6,907  -  Avg Batch Loss: 1.8070  -  Elapsed: 2:11:42\n",
      "  Batch 3,450 / 6,907  -  Avg Batch Loss: 1.8245  -  Elapsed: 2:13:33\n",
      "  Batch 3,500 / 6,907  -  Avg Batch Loss: 1.8138  -  Elapsed: 2:15:25\n",
      "  Batch 3,550 / 6,907  -  Avg Batch Loss: 1.8150  -  Elapsed: 2:17:25\n",
      "  Batch 3,600 / 6,907  -  Avg Batch Loss: 1.7978  -  Elapsed: 2:19:20\n",
      "  Batch 3,650 / 6,907  -  Avg Batch Loss: 1.8197  -  Elapsed: 2:21:15\n",
      "  Batch 3,700 / 6,907  -  Avg Batch Loss: 1.8007  -  Elapsed: 2:23:07\n",
      "  Batch 3,750 / 6,907  -  Avg Batch Loss: 1.8123  -  Elapsed: 2:24:56\n",
      "  Batch 3,800 / 6,907  -  Avg Batch Loss: 1.7948  -  Elapsed: 2:26:50\n",
      "  Batch 3,850 / 6,907  -  Avg Batch Loss: 1.7573  -  Elapsed: 2:28:49\n",
      "  Batch 3,900 / 6,907  -  Avg Batch Loss: 1.8292  -  Elapsed: 2:30:47\n",
      "  Batch 3,950 / 6,907  -  Avg Batch Loss: 1.7565  -  Elapsed: 2:32:41\n",
      "  Batch 4,000 / 6,907  -  Avg Batch Loss: 1.8416  -  Elapsed: 2:34:32\n",
      "  Batch 4,050 / 6,907  -  Avg Batch Loss: 1.8017  -  Elapsed: 2:36:25\n",
      "  Batch 4,100 / 6,907  -  Avg Batch Loss: 1.8141  -  Elapsed: 2:38:18\n",
      "  Batch 4,150 / 6,907  -  Avg Batch Loss: 1.8229  -  Elapsed: 2:40:10\n",
      "  Batch 4,200 / 6,907  -  Avg Batch Loss: 1.8176  -  Elapsed: 2:42:01\n",
      "  Batch 4,250 / 6,907  -  Avg Batch Loss: 1.7898  -  Elapsed: 2:43:52\n",
      "  Batch 4,300 / 6,907  -  Avg Batch Loss: 1.8317  -  Elapsed: 2:45:45\n",
      "  Batch 4,350 / 6,907  -  Avg Batch Loss: 1.8256  -  Elapsed: 2:47:39\n",
      "  Batch 4,400 / 6,907  -  Avg Batch Loss: 1.8333  -  Elapsed: 2:49:32\n",
      "  Batch 4,450 / 6,907  -  Avg Batch Loss: 1.8396  -  Elapsed: 2:51:28\n",
      "  Batch 4,500 / 6,907  -  Avg Batch Loss: 1.7986  -  Elapsed: 2:53:24\n",
      "  Batch 4,550 / 6,907  -  Avg Batch Loss: 1.8072  -  Elapsed: 2:55:22\n",
      "  Batch 4,600 / 6,907  -  Avg Batch Loss: 1.8335  -  Elapsed: 2:57:22\n",
      "  Batch 4,650 / 6,907  -  Avg Batch Loss: 1.8008  -  Elapsed: 2:59:21\n",
      "  Batch 4,700 / 6,907  -  Avg Batch Loss: 1.7953  -  Elapsed: 3:01:20\n",
      "  Batch 4,750 / 6,907  -  Avg Batch Loss: 1.8189  -  Elapsed: 3:03:19\n",
      "  Batch 4,800 / 6,907  -  Avg Batch Loss: 1.7751  -  Elapsed: 3:05:19\n",
      "  Batch 4,850 / 6,907  -  Avg Batch Loss: 1.7806  -  Elapsed: 3:07:19\n",
      "  Batch 4,900 / 6,907  -  Avg Batch Loss: 1.7932  -  Elapsed: 3:09:18\n",
      "  Batch 4,950 / 6,907  -  Avg Batch Loss: 1.8502  -  Elapsed: 3:11:16\n",
      "  Batch 5,000 / 6,907  -  Avg Batch Loss: 1.8049  -  Elapsed: 3:13:14\n",
      "  Batch 5,050 / 6,907  -  Avg Batch Loss: 1.8289  -  Elapsed: 3:15:12\n",
      "  Batch 5,100 / 6,907  -  Avg Batch Loss: 1.8169  -  Elapsed: 3:17:09\n",
      "  Batch 5,150 / 6,907  -  Avg Batch Loss: 1.8110  -  Elapsed: 3:19:05\n",
      "  Batch 5,200 / 6,907  -  Avg Batch Loss: 1.7698  -  Elapsed: 3:21:02\n",
      "  Batch 5,250 / 6,907  -  Avg Batch Loss: 1.8533  -  Elapsed: 3:22:59\n",
      "  Batch 5,300 / 6,907  -  Avg Batch Loss: 1.8090  -  Elapsed: 3:24:55\n",
      "  Batch 5,350 / 6,907  -  Avg Batch Loss: 1.7999  -  Elapsed: 3:26:53\n",
      "  Batch 5,400 / 6,907  -  Avg Batch Loss: 1.8157  -  Elapsed: 3:28:49\n",
      "  Batch 5,450 / 6,907  -  Avg Batch Loss: 1.7766  -  Elapsed: 3:30:44\n",
      "  Batch 5,500 / 6,907  -  Avg Batch Loss: 1.7905  -  Elapsed: 3:32:34\n",
      "  Batch 5,550 / 6,907  -  Avg Batch Loss: 1.8239  -  Elapsed: 3:34:24\n",
      "  Batch 5,600 / 6,907  -  Avg Batch Loss: 1.8185  -  Elapsed: 3:36:16\n",
      "  Batch 5,650 / 6,907  -  Avg Batch Loss: 1.8053  -  Elapsed: 3:38:07\n",
      "  Batch 5,700 / 6,907  -  Avg Batch Loss: 1.8048  -  Elapsed: 3:39:58\n",
      "  Batch 5,750 / 6,907  -  Avg Batch Loss: 1.8243  -  Elapsed: 3:41:49\n",
      "  Batch 5,800 / 6,907  -  Avg Batch Loss: 1.8162  -  Elapsed: 3:43:42\n",
      "  Batch 5,850 / 6,907  -  Avg Batch Loss: 1.7983  -  Elapsed: 3:45:34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,900 / 6,907  -  Avg Batch Loss: 1.7577  -  Elapsed: 3:47:26\n",
      "  Batch 5,950 / 6,907  -  Avg Batch Loss: 1.8380  -  Elapsed: 3:49:18\n",
      "  Batch 6,000 / 6,907  -  Avg Batch Loss: 1.8041  -  Elapsed: 3:51:16\n",
      "  Batch 6,050 / 6,907  -  Avg Batch Loss: 1.7747  -  Elapsed: 3:53:15\n",
      "  Batch 6,100 / 6,907  -  Avg Batch Loss: 1.8216  -  Elapsed: 3:55:14\n",
      "  Batch 6,150 / 6,907  -  Avg Batch Loss: 1.7852  -  Elapsed: 3:57:12\n",
      "  Batch 6,200 / 6,907  -  Avg Batch Loss: 1.7772  -  Elapsed: 3:59:10\n",
      "  Batch 6,250 / 6,907  -  Avg Batch Loss: 1.7649  -  Elapsed: 4:01:02\n",
      "  Batch 6,300 / 6,907  -  Avg Batch Loss: 1.8223  -  Elapsed: 4:02:55\n",
      "  Batch 6,350 / 6,907  -  Avg Batch Loss: 1.8206  -  Elapsed: 4:04:48\n",
      "  Batch 6,400 / 6,907  -  Avg Batch Loss: 1.7965  -  Elapsed: 4:06:42\n",
      "  Batch 6,450 / 6,907  -  Avg Batch Loss: 1.8076  -  Elapsed: 4:08:36\n",
      "  Batch 6,500 / 6,907  -  Avg Batch Loss: 1.8294  -  Elapsed: 4:10:31\n",
      "  Batch 6,550 / 6,907  -  Avg Batch Loss: 1.7680  -  Elapsed: 4:12:27\n",
      "  Batch 6,600 / 6,907  -  Avg Batch Loss: 1.7676  -  Elapsed: 4:14:22\n",
      "  Batch 6,650 / 6,907  -  Avg Batch Loss: 1.7263  -  Elapsed: 4:16:18\n",
      "  Batch 6,700 / 6,907  -  Avg Batch Loss: 1.8340  -  Elapsed: 4:18:14\n",
      "  Batch 6,750 / 6,907  -  Avg Batch Loss: 1.8276  -  Elapsed: 4:20:10\n",
      "  Batch 6,800 / 6,907  -  Avg Batch Loss: 1.8089  -  Elapsed: 4:22:11\n",
      "  Batch 6,850 / 6,907  -  Avg Batch Loss: 1.8114  -  Elapsed: 4:24:06\n",
      "  Batch 6,900 / 6,907  -  Avg Batch Loss: 1.7564  -  Elapsed: 4:26:03\n",
      "\n",
      "  Average training loss: 1.81\n",
      "  Training epoch took: 4:26:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.20\n",
      "  Validation Loss: 1.79\n",
      "  Validation F1 Score: 0.198\n",
      "  Validation took: 0:07:33\n",
      "\n",
      "Training complete!\n",
      "Total training took 4:33:52 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "stats_roberta_custom_2 = BERT_fine_tune_train(roberta_custom_wgt_cel,\n",
    "                    train_dataloader,\n",
    "                    validation_dataloader,\n",
    "                    device = device,\n",
    "                    metric = metric,\n",
    "                    optimizer = optim,\n",
    "                    scheduler = scheduler,\n",
    "                    epochs = epochs,\n",
    "                    lr = lr,\n",
    "                    n_warmup = n_warmup\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "6b5977e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(stats_roberta_custom_2[0]['val_stats']['y_true'], \n",
    "                      stats_roberta_custom_2[0]['val_stats']['y_pred'], \n",
    "                      idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ced242b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RobertaClassifier' from 'transformers.modeling_outputs' (/Users/paulp/miniforge3/envs/pyto/lib/python3.9/site-packages/transformers/modeling_outputs.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequenceClassifierOutput, RobertaClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBertClassifierCustom\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RobertaClassifier' from 'transformers.modeling_outputs' (/Users/paulp/miniforge3/envs/pyto/lib/python3.9/site-packages/transformers/modeling_outputs.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertClassifierCustom(nn.Module):\n",
    "    '''\n",
    "    Custom output layer     \n",
    "    '''\n",
    "    def __init__(self, model, loss_fn, classifier_head, n_classes):\n",
    "\n",
    "        super(BertClassifierCustom, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_labels = n_classes\n",
    "        self.linear_layer = nn.Linear(in_features = 768, out_features = 768)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "        self.classifier = nn.Linear(768, self.num_labels)\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def forward(self, ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = self.model(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        #self.classifier\n",
    "        #output = self.dropout(outputs.pooler_output)\n",
    "        logits = outputs.logits\n",
    "        linear_layer(outputs)\n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            # you can define any loss function here yourself\n",
    "            # see https://pytorch.org/docs/stable/nn.html#loss-functions for an overview\n",
    "            #loss_fct = focal_loss\n",
    "            # next, compute the loss based on logits + ground-truth labels\n",
    "        #loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "631b2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_cel = nn.CrossEntropyLoss(weights.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "80ca7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_cls_weighted_loss_mdl = BertClassifierCustom(roberta_model, loss_fn=weighted_cel, n_classes=n_classes)\n",
    "bert_cls_weighted_loss_mdl = BertClassifierCustom(bert_model, loss_fn = weighted_cel, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d2d7237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "batch_size = 2\n",
    "lr = 1e-5\n",
    "n_warmup = 0\n",
    "epochs = 1\n",
    "#w_d = 0.0001\n",
    "use_mps = torch.has_mps\n",
    "#device = torch.device('mps' if use_mps else 'cpu') STILL not working\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3c095ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(ds_tr, roberta_tokenizer, target_idx, max_seq_len=max_len)\n",
    "val_dataset = ClassificationDataset(ds_vl, roberta_tokenizer, target_idx, max_seq_len=max_len)\n",
    "test_dataset = ClassificationDataset(ds_ts, roberta_tokenizer, target_idx, max_seq_len=max_len)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    sampler = RandomSampler(train_dataset), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler = RandomSampler(val_dataset),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler = RandomSampler(test_dataset),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e751ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader)*epochs\n",
    "\n",
    "optim = AdamW(roberta_cls_weighted_loss_mdl.parameters(), lr=lr)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                           num_warmup_steps = n_warmup,\n",
    "                                           num_training_steps = total_steps)\n",
    "\n",
    "metric = F1Score(num_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "61fb9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(common)\n",
    "from common import BERT_fine_tune_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b144e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    50 / 6,907  -  Avg Batch Loss: 1.7652  -  Elapsed: 0:01:42\n",
      "  Batch   100 / 6,907  -  Avg Batch Loss: 1.7465  -  Elapsed: 0:03:25\n",
      "  Batch   150 / 6,907  -  Avg Batch Loss: 1.8245  -  Elapsed: 0:05:06\n",
      "  Batch   200 / 6,907  -  Avg Batch Loss: 1.7779  -  Elapsed: 0:06:48\n",
      "  Batch   250 / 6,907  -  Avg Batch Loss: 1.7550  -  Elapsed: 0:08:29\n",
      "  Batch   300 / 6,907  -  Avg Batch Loss: 1.8274  -  Elapsed: 0:10:11\n",
      "  Batch   350 / 6,907  -  Avg Batch Loss: 1.7829  -  Elapsed: 0:11:55\n",
      "  Batch   400 / 6,907  -  Avg Batch Loss: 1.7707  -  Elapsed: 0:13:37\n",
      "  Batch   450 / 6,907  -  Avg Batch Loss: 1.7904  -  Elapsed: 0:15:20\n",
      "  Batch   500 / 6,907  -  Avg Batch Loss: 1.7571  -  Elapsed: 0:17:04\n",
      "  Batch   550 / 6,907  -  Avg Batch Loss: 1.7895  -  Elapsed: 0:18:47\n",
      "  Batch   600 / 6,907  -  Avg Batch Loss: 1.7951  -  Elapsed: 0:20:31\n",
      "  Batch   650 / 6,907  -  Avg Batch Loss: 1.7641  -  Elapsed: 0:22:16\n",
      "  Batch   700 / 6,907  -  Avg Batch Loss: 1.8053  -  Elapsed: 0:23:59\n",
      "  Batch   750 / 6,907  -  Avg Batch Loss: 1.7868  -  Elapsed: 0:25:43\n",
      "  Batch   800 / 6,907  -  Avg Batch Loss: 1.7924  -  Elapsed: 0:27:26\n",
      "  Batch   850 / 6,907  -  Avg Batch Loss: 1.8173  -  Elapsed: 0:29:09\n",
      "  Batch   900 / 6,907  -  Avg Batch Loss: 1.7698  -  Elapsed: 0:30:52\n",
      "  Batch   950 / 6,907  -  Avg Batch Loss: 1.7666  -  Elapsed: 0:32:38\n",
      "  Batch 1,000 / 6,907  -  Avg Batch Loss: 1.7734  -  Elapsed: 0:34:22\n",
      "  Batch 1,050 / 6,907  -  Avg Batch Loss: 1.8010  -  Elapsed: 0:36:16\n",
      "  Batch 1,100 / 6,907  -  Avg Batch Loss: 1.8039  -  Elapsed: 0:38:08\n",
      "  Batch 1,150 / 6,907  -  Avg Batch Loss: 1.7933  -  Elapsed: 0:40:01\n",
      "  Batch 1,200 / 6,907  -  Avg Batch Loss: 1.7855  -  Elapsed: 0:41:55\n",
      "  Batch 1,250 / 6,907  -  Avg Batch Loss: 1.7873  -  Elapsed: 0:43:51\n",
      "  Batch 1,300 / 6,907  -  Avg Batch Loss: 1.8023  -  Elapsed: 0:45:52\n",
      "  Batch 1,350 / 6,907  -  Avg Batch Loss: 1.8153  -  Elapsed: 0:47:44\n",
      "  Batch 1,400 / 6,907  -  Avg Batch Loss: 1.7901  -  Elapsed: 0:49:39\n",
      "  Batch 1,450 / 6,907  -  Avg Batch Loss: 1.7911  -  Elapsed: 0:51:42\n",
      "  Batch 1,500 / 6,907  -  Avg Batch Loss: 1.7932  -  Elapsed: 0:53:37\n",
      "  Batch 1,550 / 6,907  -  Avg Batch Loss: 1.7627  -  Elapsed: 0:55:29\n",
      "  Batch 1,600 / 6,907  -  Avg Batch Loss: 1.7736  -  Elapsed: 0:57:15\n",
      "  Batch 1,650 / 6,907  -  Avg Batch Loss: 1.7923  -  Elapsed: 0:58:58\n",
      "  Batch 1,700 / 6,907  -  Avg Batch Loss: 1.8046  -  Elapsed: 1:00:40\n",
      "  Batch 1,750 / 6,907  -  Avg Batch Loss: 1.7626  -  Elapsed: 1:02:22\n",
      "  Batch 1,800 / 6,907  -  Avg Batch Loss: 1.7940  -  Elapsed: 1:04:04\n",
      "  Batch 1,850 / 6,907  -  Avg Batch Loss: 1.7553  -  Elapsed: 1:05:45\n",
      "  Batch 1,900 / 6,907  -  Avg Batch Loss: 1.7929  -  Elapsed: 1:07:28\n",
      "  Batch 1,950 / 6,907  -  Avg Batch Loss: 1.8079  -  Elapsed: 1:09:10\n",
      "  Batch 2,000 / 6,907  -  Avg Batch Loss: 1.7853  -  Elapsed: 1:10:53\n",
      "  Batch 2,050 / 6,907  -  Avg Batch Loss: 1.7955  -  Elapsed: 1:12:34\n",
      "  Batch 2,100 / 6,907  -  Avg Batch Loss: 1.7690  -  Elapsed: 1:14:13\n",
      "  Batch 2,150 / 6,907  -  Avg Batch Loss: 1.7697  -  Elapsed: 1:15:53\n",
      "  Batch 2,200 / 6,907  -  Avg Batch Loss: 1.8514  -  Elapsed: 1:17:33\n",
      "  Batch 2,250 / 6,907  -  Avg Batch Loss: 1.7916  -  Elapsed: 1:19:13\n",
      "  Batch 2,300 / 6,907  -  Avg Batch Loss: 1.7578  -  Elapsed: 1:20:53\n",
      "  Batch 2,350 / 6,907  -  Avg Batch Loss: 1.8114  -  Elapsed: 1:22:33\n",
      "  Batch 2,400 / 6,907  -  Avg Batch Loss: 1.8000  -  Elapsed: 1:24:13\n",
      "  Batch 2,450 / 6,907  -  Avg Batch Loss: 1.7883  -  Elapsed: 1:25:53\n",
      "  Batch 2,500 / 6,907  -  Avg Batch Loss: 1.7857  -  Elapsed: 1:27:32\n",
      "  Batch 2,550 / 6,907  -  Avg Batch Loss: 1.8229  -  Elapsed: 1:29:12\n",
      "  Batch 2,600 / 6,907  -  Avg Batch Loss: 1.7719  -  Elapsed: 1:30:52\n",
      "  Batch 2,650 / 6,907  -  Avg Batch Loss: 1.7898  -  Elapsed: 1:32:32\n",
      "  Batch 2,700 / 6,907  -  Avg Batch Loss: 1.7776  -  Elapsed: 1:34:12\n",
      "  Batch 2,750 / 6,907  -  Avg Batch Loss: 1.7700  -  Elapsed: 1:35:51\n",
      "  Batch 2,800 / 6,907  -  Avg Batch Loss: 1.7728  -  Elapsed: 1:37:31\n",
      "  Batch 2,850 / 6,907  -  Avg Batch Loss: 1.8143  -  Elapsed: 1:39:11\n",
      "  Batch 2,900 / 6,907  -  Avg Batch Loss: 1.7778  -  Elapsed: 1:40:51\n",
      "  Batch 2,950 / 6,907  -  Avg Batch Loss: 1.7872  -  Elapsed: 1:42:32\n",
      "  Batch 3,000 / 6,907  -  Avg Batch Loss: 1.8336  -  Elapsed: 1:44:14\n",
      "  Batch 3,050 / 6,907  -  Avg Batch Loss: 1.7726  -  Elapsed: 1:45:56\n",
      "  Batch 3,100 / 6,907  -  Avg Batch Loss: 1.7271  -  Elapsed: 1:47:40\n",
      "  Batch 3,150 / 6,907  -  Avg Batch Loss: 1.8177  -  Elapsed: 1:49:24\n",
      "  Batch 3,200 / 6,907  -  Avg Batch Loss: 1.8263  -  Elapsed: 1:51:07\n",
      "  Batch 3,250 / 6,907  -  Avg Batch Loss: 1.8174  -  Elapsed: 1:52:51\n",
      "  Batch 3,300 / 6,907  -  Avg Batch Loss: 1.7830  -  Elapsed: 1:54:34\n",
      "  Batch 3,350 / 6,907  -  Avg Batch Loss: 1.7817  -  Elapsed: 1:56:18\n",
      "  Batch 3,400 / 6,907  -  Avg Batch Loss: 1.8029  -  Elapsed: 1:58:01\n",
      "  Batch 3,450 / 6,907  -  Avg Batch Loss: 1.8079  -  Elapsed: 1:59:45\n",
      "  Batch 3,500 / 6,907  -  Avg Batch Loss: 1.7974  -  Elapsed: 2:01:29\n",
      "  Batch 3,550 / 6,907  -  Avg Batch Loss: 1.7791  -  Elapsed: 2:03:13\n",
      "  Batch 3,600 / 6,907  -  Avg Batch Loss: 1.7854  -  Elapsed: 2:04:56\n",
      "  Batch 3,650 / 6,907  -  Avg Batch Loss: 1.7898  -  Elapsed: 2:06:39\n",
      "  Batch 3,700 / 6,907  -  Avg Batch Loss: 1.8155  -  Elapsed: 2:08:23\n",
      "  Batch 3,750 / 6,907  -  Avg Batch Loss: 1.7801  -  Elapsed: 2:10:04\n",
      "  Batch 3,800 / 6,907  -  Avg Batch Loss: 1.8117  -  Elapsed: 2:11:46\n",
      "  Batch 3,850 / 6,907  -  Avg Batch Loss: 1.7601  -  Elapsed: 2:13:27\n",
      "  Batch 3,900 / 6,907  -  Avg Batch Loss: 1.7848  -  Elapsed: 2:15:07\n",
      "  Batch 3,950 / 6,907  -  Avg Batch Loss: 1.7859  -  Elapsed: 2:16:48\n",
      "  Batch 4,000 / 6,907  -  Avg Batch Loss: 1.7824  -  Elapsed: 2:18:28\n",
      "  Batch 4,050 / 6,907  -  Avg Batch Loss: 1.7651  -  Elapsed: 2:20:10\n",
      "  Batch 4,100 / 6,907  -  Avg Batch Loss: 1.8090  -  Elapsed: 2:21:50\n",
      "  Batch 4,150 / 6,907  -  Avg Batch Loss: 1.7766  -  Elapsed: 2:23:31\n",
      "  Batch 4,200 / 6,907  -  Avg Batch Loss: 1.7872  -  Elapsed: 2:25:13\n",
      "  Batch 4,250 / 6,907  -  Avg Batch Loss: 1.7765  -  Elapsed: 2:26:54\n",
      "  Batch 4,300 / 6,907  -  Avg Batch Loss: 1.7658  -  Elapsed: 2:28:36\n",
      "  Batch 4,350 / 6,907  -  Avg Batch Loss: 1.7768  -  Elapsed: 2:30:21\n",
      "  Batch 4,400 / 6,907  -  Avg Batch Loss: 1.8390  -  Elapsed: 2:32:01\n",
      "  Batch 4,450 / 6,907  -  Avg Batch Loss: 1.7950  -  Elapsed: 2:33:41\n",
      "  Batch 4,500 / 6,907  -  Avg Batch Loss: 1.7874  -  Elapsed: 2:35:22\n",
      "  Batch 4,550 / 6,907  -  Avg Batch Loss: 1.7456  -  Elapsed: 2:37:02\n",
      "  Batch 4,600 / 6,907  -  Avg Batch Loss: 1.7602  -  Elapsed: 2:38:43\n",
      "  Batch 4,650 / 6,907  -  Avg Batch Loss: 1.7992  -  Elapsed: 2:40:23\n",
      "  Batch 4,700 / 6,907  -  Avg Batch Loss: 1.7652  -  Elapsed: 2:42:04\n",
      "  Batch 4,750 / 6,907  -  Avg Batch Loss: 1.7728  -  Elapsed: 2:43:45\n",
      "  Batch 4,800 / 6,907  -  Avg Batch Loss: 1.7422  -  Elapsed: 2:45:26\n",
      "  Batch 4,850 / 6,907  -  Avg Batch Loss: 1.8125  -  Elapsed: 2:47:08\n",
      "  Batch 4,900 / 6,907  -  Avg Batch Loss: 1.8066  -  Elapsed: 2:48:52\n",
      "  Batch 4,950 / 6,907  -  Avg Batch Loss: 1.7730  -  Elapsed: 2:50:35\n",
      "  Batch 5,000 / 6,907  -  Avg Batch Loss: 1.7898  -  Elapsed: 2:52:17\n",
      "  Batch 5,050 / 6,907  -  Avg Batch Loss: 1.7660  -  Elapsed: 2:54:00\n",
      "  Batch 5,100 / 6,907  -  Avg Batch Loss: 1.8107  -  Elapsed: 2:55:43\n",
      "  Batch 5,150 / 6,907  -  Avg Batch Loss: 1.7403  -  Elapsed: 2:57:26\n",
      "  Batch 5,200 / 6,907  -  Avg Batch Loss: 1.7785  -  Elapsed: 2:59:08\n",
      "  Batch 5,250 / 6,907  -  Avg Batch Loss: 1.7781  -  Elapsed: 3:00:51\n",
      "  Batch 5,300 / 6,907  -  Avg Batch Loss: 1.7842  -  Elapsed: 3:02:34\n",
      "  Batch 5,350 / 6,907  -  Avg Batch Loss: 1.7092  -  Elapsed: 3:04:17\n",
      "  Batch 5,400 / 6,907  -  Avg Batch Loss: 1.7880  -  Elapsed: 3:05:59\n",
      "  Batch 5,450 / 6,907  -  Avg Batch Loss: 1.7165  -  Elapsed: 3:07:42\n",
      "  Batch 5,500 / 6,907  -  Avg Batch Loss: 1.8076  -  Elapsed: 3:09:24\n",
      "  Batch 5,550 / 6,907  -  Avg Batch Loss: 1.7676  -  Elapsed: 3:11:07\n",
      "  Batch 5,600 / 6,907  -  Avg Batch Loss: 1.8170  -  Elapsed: 3:12:51\n",
      "  Batch 5,650 / 6,907  -  Avg Batch Loss: 1.7961  -  Elapsed: 3:14:33\n",
      "  Batch 5,700 / 6,907  -  Avg Batch Loss: 1.7671  -  Elapsed: 3:16:15\n",
      "  Batch 5,750 / 6,907  -  Avg Batch Loss: 1.7444  -  Elapsed: 3:17:57\n",
      "  Batch 5,800 / 6,907  -  Avg Batch Loss: 1.8001  -  Elapsed: 3:19:39\n",
      "  Batch 5,850 / 6,907  -  Avg Batch Loss: 1.8421  -  Elapsed: 3:21:21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,900 / 6,907  -  Avg Batch Loss: 1.7920  -  Elapsed: 3:23:02\n",
      "  Batch 5,950 / 6,907  -  Avg Batch Loss: 1.7814  -  Elapsed: 3:24:44\n",
      "  Batch 6,000 / 6,907  -  Avg Batch Loss: 1.7600  -  Elapsed: 3:26:26\n",
      "  Batch 6,050 / 6,907  -  Avg Batch Loss: 1.7497  -  Elapsed: 3:28:08\n",
      "  Batch 6,100 / 6,907  -  Avg Batch Loss: 1.7306  -  Elapsed: 3:29:48\n",
      "  Batch 6,150 / 6,907  -  Avg Batch Loss: 1.7360  -  Elapsed: 3:31:28\n",
      "  Batch 6,200 / 6,907  -  Avg Batch Loss: 1.8122  -  Elapsed: 3:33:08\n",
      "  Batch 6,250 / 6,907  -  Avg Batch Loss: 1.7793  -  Elapsed: 3:34:48\n",
      "  Batch 6,300 / 6,907  -  Avg Batch Loss: 1.8062  -  Elapsed: 3:36:28\n",
      "  Batch 6,350 / 6,907  -  Avg Batch Loss: 1.7834  -  Elapsed: 3:38:09\n",
      "  Batch 6,400 / 6,907  -  Avg Batch Loss: 1.7715  -  Elapsed: 3:39:50\n",
      "  Batch 6,450 / 6,907  -  Avg Batch Loss: 1.7803  -  Elapsed: 3:41:31\n",
      "  Batch 6,500 / 6,907  -  Avg Batch Loss: 1.8069  -  Elapsed: 3:43:11\n",
      "  Batch 6,550 / 6,907  -  Avg Batch Loss: 1.7834  -  Elapsed: 3:44:51\n",
      "  Batch 6,600 / 6,907  -  Avg Batch Loss: 1.7949  -  Elapsed: 3:46:31\n",
      "  Batch 6,650 / 6,907  -  Avg Batch Loss: 1.7735  -  Elapsed: 3:48:11\n",
      "  Batch 6,700 / 6,907  -  Avg Batch Loss: 1.7996  -  Elapsed: 3:49:51\n",
      "  Batch 6,750 / 6,907  -  Avg Batch Loss: 1.7702  -  Elapsed: 3:51:31\n",
      "  Batch 6,800 / 6,907  -  Avg Batch Loss: 1.7613  -  Elapsed: 3:53:11\n",
      "  Batch 6,850 / 6,907  -  Avg Batch Loss: 1.7837  -  Elapsed: 3:54:51\n",
      "  Batch 6,900 / 6,907  -  Avg Batch Loss: 1.7878  -  Elapsed: 3:56:32\n",
      "\n",
      "  Average training loss: 1.78\n",
      "  Training epoch took: 3:56:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.29\n",
      "  Validation Loss: 1.77\n",
      "  Validation F1 Score: 0.293\n",
      "  Validation took: 0:06:14\n",
      "\n",
      "Training complete!\n",
      "Total training took 4:03:00 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "stats_roberta = BERT_fine_tune_train(roberta_cls_weighted_loss_mdl,\n",
    "                    train_dataloader,\n",
    "                    validation_dataloader,\n",
    "                    device = device,\n",
    "                    metric = metric,\n",
    "                    optimizer = optim,\n",
    "                    scheduler = scheduler,\n",
    "                    epochs = epochs,\n",
    "                    lr = lr,\n",
    "                    n_warmup = n_warmup\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0af75ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(stats_roberta[0]['val_stats']['y_true'], \n",
    "                      stats_roberta[0]['val_stats']['y_pred'], \n",
    "                      idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fb069ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8507264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'val_stats': {'avg_val_loss': 1.7984882758189273,\n",
       "   'val_time': '0:06:16',\n",
       "   'val_f1': tensor(0.1029),\n",
       "   'avg_flat_acc': 0.1028225806451613,\n",
       "   'y_pred': tensor([5, 5, 5,  ..., 5, 5, 5], dtype=torch.int32),\n",
       "   'y_true': tensor([3, 2, 3,  ..., 2, 3, 0], dtype=torch.int32)},\n",
       "  'train_stats': {'avg_train_loss': 1.7987718363651857,\n",
       "   'training_time': '6:15:42'}}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee857b",
   "metadata": {},
   "source": [
    "<h2> BERT </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5a8575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(ds_tr, bert_tokenizer, target_idx, max_seq_len=max_len)\n",
    "val_dataset = ClassificationDataset(ds_vl, bert_tokenizer, target_idx, max_seq_len=max_len)\n",
    "test_dataset = ClassificationDataset(ds_ts, bert_tokenizer, target_idx, max_seq_len=max_len)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    sampler = RandomSampler(train_dataset), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler = RandomSampler(val_dataset),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler = RandomSampler(test_dataset),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b0efbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(bert_cls_weighted_loss_mdl.parameters(), lr=lr)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                           num_warmup_steps = n_warmup,\n",
    "                                           num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8b67b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    50 / 6,907  -  Avg Batch Loss: 1.8472  -  Elapsed: 0:01:38\n",
      "  Batch   100 / 6,907  -  Avg Batch Loss: 1.8631  -  Elapsed: 0:03:17\n",
      "  Batch   150 / 6,907  -  Avg Batch Loss: 1.7984  -  Elapsed: 0:04:55\n",
      "  Batch   200 / 6,907  -  Avg Batch Loss: 1.8306  -  Elapsed: 0:06:33\n",
      "  Batch   250 / 6,907  -  Avg Batch Loss: 1.8026  -  Elapsed: 0:08:12\n",
      "  Batch   300 / 6,907  -  Avg Batch Loss: 1.8317  -  Elapsed: 0:09:50\n",
      "  Batch   350 / 6,907  -  Avg Batch Loss: 1.7656  -  Elapsed: 0:11:29\n",
      "  Batch   400 / 6,907  -  Avg Batch Loss: 1.8420  -  Elapsed: 0:13:07\n",
      "  Batch   450 / 6,907  -  Avg Batch Loss: 1.8220  -  Elapsed: 0:14:46\n",
      "  Batch   500 / 6,907  -  Avg Batch Loss: 1.7931  -  Elapsed: 0:16:24\n",
      "  Batch   550 / 6,907  -  Avg Batch Loss: 1.8242  -  Elapsed: 0:18:03\n",
      "  Batch   600 / 6,907  -  Avg Batch Loss: 1.8442  -  Elapsed: 0:19:42\n",
      "  Batch   650 / 6,907  -  Avg Batch Loss: 1.7961  -  Elapsed: 0:21:26\n",
      "  Batch   700 / 6,907  -  Avg Batch Loss: 1.7918  -  Elapsed: 0:23:09\n",
      "  Batch   750 / 6,907  -  Avg Batch Loss: 1.7912  -  Elapsed: 0:24:53\n",
      "  Batch   800 / 6,907  -  Avg Batch Loss: 1.8243  -  Elapsed: 0:26:36\n",
      "  Batch   850 / 6,907  -  Avg Batch Loss: 1.7984  -  Elapsed: 0:28:20\n",
      "  Batch   900 / 6,907  -  Avg Batch Loss: 1.8251  -  Elapsed: 0:30:02\n",
      "  Batch   950 / 6,907  -  Avg Batch Loss: 1.7719  -  Elapsed: 0:31:45\n",
      "  Batch 1,000 / 6,907  -  Avg Batch Loss: 1.7409  -  Elapsed: 0:33:25\n",
      "  Batch 1,050 / 6,907  -  Avg Batch Loss: 1.7891  -  Elapsed: 0:35:08\n",
      "  Batch 1,100 / 6,907  -  Avg Batch Loss: 1.8111  -  Elapsed: 0:36:50\n",
      "  Batch 1,150 / 6,907  -  Avg Batch Loss: 1.8149  -  Elapsed: 0:38:34\n",
      "  Batch 1,200 / 6,907  -  Avg Batch Loss: 1.8067  -  Elapsed: 0:40:21\n",
      "  Batch 1,250 / 6,907  -  Avg Batch Loss: 1.7956  -  Elapsed: 0:42:07\n",
      "  Batch 1,300 / 6,907  -  Avg Batch Loss: 1.8102  -  Elapsed: 0:43:51\n",
      "  Batch 1,350 / 6,907  -  Avg Batch Loss: 1.8065  -  Elapsed: 0:45:37\n",
      "  Batch 1,400 / 6,907  -  Avg Batch Loss: 1.7728  -  Elapsed: 0:47:25\n",
      "  Batch 1,450 / 6,907  -  Avg Batch Loss: 1.7700  -  Elapsed: 0:49:10\n",
      "  Batch 1,500 / 6,907  -  Avg Batch Loss: 1.8410  -  Elapsed: 0:50:51\n",
      "  Batch 1,550 / 6,907  -  Avg Batch Loss: 1.8079  -  Elapsed: 0:52:31\n",
      "  Batch 1,600 / 6,907  -  Avg Batch Loss: 1.7981  -  Elapsed: 0:54:11\n",
      "  Batch 1,650 / 6,907  -  Avg Batch Loss: 1.7712  -  Elapsed: 0:55:52\n",
      "  Batch 1,700 / 6,907  -  Avg Batch Loss: 1.8139  -  Elapsed: 0:57:35\n",
      "  Batch 1,750 / 6,907  -  Avg Batch Loss: 1.8045  -  Elapsed: 0:59:18\n",
      "  Batch 1,800 / 6,907  -  Avg Batch Loss: 1.8122  -  Elapsed: 1:01:00\n",
      "  Batch 1,850 / 6,907  -  Avg Batch Loss: 1.8028  -  Elapsed: 1:02:42\n",
      "  Batch 1,900 / 6,907  -  Avg Batch Loss: 1.8028  -  Elapsed: 1:04:25\n",
      "  Batch 1,950 / 6,907  -  Avg Batch Loss: 1.7435  -  Elapsed: 1:06:05\n",
      "  Batch 2,000 / 6,907  -  Avg Batch Loss: 1.8082  -  Elapsed: 1:07:47\n",
      "  Batch 2,050 / 6,907  -  Avg Batch Loss: 1.8137  -  Elapsed: 1:09:28\n",
      "  Batch 2,100 / 6,907  -  Avg Batch Loss: 1.7633  -  Elapsed: 1:11:11\n",
      "  Batch 2,150 / 6,907  -  Avg Batch Loss: 1.8082  -  Elapsed: 1:12:53\n",
      "  Batch 2,200 / 6,907  -  Avg Batch Loss: 1.7905  -  Elapsed: 1:14:35\n",
      "  Batch 2,250 / 6,907  -  Avg Batch Loss: 1.8025  -  Elapsed: 1:16:15\n",
      "  Batch 2,300 / 6,907  -  Avg Batch Loss: 1.7985  -  Elapsed: 1:17:57\n",
      "  Batch 2,350 / 6,907  -  Avg Batch Loss: 1.7927  -  Elapsed: 1:19:40\n",
      "  Batch 2,400 / 6,907  -  Avg Batch Loss: 1.8500  -  Elapsed: 1:21:22\n",
      "  Batch 2,450 / 6,907  -  Avg Batch Loss: 1.7875  -  Elapsed: 1:23:04\n",
      "  Batch 2,500 / 6,907  -  Avg Batch Loss: 1.8249  -  Elapsed: 1:24:46\n",
      "  Batch 2,550 / 6,907  -  Avg Batch Loss: 1.7889  -  Elapsed: 1:26:28\n",
      "  Batch 2,600 / 6,907  -  Avg Batch Loss: 1.8017  -  Elapsed: 1:28:10\n",
      "  Batch 2,650 / 6,907  -  Avg Batch Loss: 1.7658  -  Elapsed: 1:29:51\n",
      "  Batch 2,700 / 6,907  -  Avg Batch Loss: 1.7974  -  Elapsed: 1:31:31\n",
      "  Batch 2,750 / 6,907  -  Avg Batch Loss: 1.8258  -  Elapsed: 1:33:11\n",
      "  Batch 2,800 / 6,907  -  Avg Batch Loss: 1.8006  -  Elapsed: 1:34:51\n",
      "  Batch 2,850 / 6,907  -  Avg Batch Loss: 1.8016  -  Elapsed: 1:36:31\n",
      "  Batch 2,900 / 6,907  -  Avg Batch Loss: 1.7876  -  Elapsed: 1:38:11\n",
      "  Batch 2,950 / 6,907  -  Avg Batch Loss: 1.8022  -  Elapsed: 1:39:56\n",
      "  Batch 3,000 / 6,907  -  Avg Batch Loss: 1.7844  -  Elapsed: 1:41:44\n",
      "  Batch 3,050 / 6,907  -  Avg Batch Loss: 1.7679  -  Elapsed: 1:43:40\n",
      "  Batch 3,100 / 6,907  -  Avg Batch Loss: 1.8534  -  Elapsed: 1:45:36\n",
      "  Batch 3,150 / 6,907  -  Avg Batch Loss: 1.7911  -  Elapsed: 1:47:29\n",
      "  Batch 3,200 / 6,907  -  Avg Batch Loss: 1.7595  -  Elapsed: 1:49:23\n",
      "  Batch 3,250 / 6,907  -  Avg Batch Loss: 1.7696  -  Elapsed: 1:51:17\n",
      "  Batch 3,300 / 6,907  -  Avg Batch Loss: 1.8182  -  Elapsed: 1:53:12\n",
      "  Batch 3,350 / 6,907  -  Avg Batch Loss: 1.7677  -  Elapsed: 1:55:01\n",
      "  Batch 3,400 / 6,907  -  Avg Batch Loss: 1.7924  -  Elapsed: 1:56:51\n",
      "  Batch 3,450 / 6,907  -  Avg Batch Loss: 1.7710  -  Elapsed: 1:58:41\n",
      "  Batch 3,500 / 6,907  -  Avg Batch Loss: 1.7662  -  Elapsed: 2:00:33\n",
      "  Batch 3,550 / 6,907  -  Avg Batch Loss: 1.7431  -  Elapsed: 2:02:21\n",
      "  Batch 3,600 / 6,907  -  Avg Batch Loss: 1.7985  -  Elapsed: 2:04:13\n",
      "  Batch 3,650 / 6,907  -  Avg Batch Loss: 1.7838  -  Elapsed: 2:06:01\n",
      "  Batch 3,700 / 6,907  -  Avg Batch Loss: 1.8301  -  Elapsed: 2:07:53\n",
      "  Batch 3,750 / 6,907  -  Avg Batch Loss: 1.7534  -  Elapsed: 2:09:42\n",
      "  Batch 3,800 / 6,907  -  Avg Batch Loss: 1.8069  -  Elapsed: 2:11:32\n",
      "  Batch 3,850 / 6,907  -  Avg Batch Loss: 1.8069  -  Elapsed: 2:13:33\n",
      "  Batch 3,900 / 6,907  -  Avg Batch Loss: 1.7940  -  Elapsed: 2:15:44\n",
      "  Batch 3,950 / 6,907  -  Avg Batch Loss: 1.7695  -  Elapsed: 2:17:38\n",
      "  Batch 4,000 / 6,907  -  Avg Batch Loss: 1.7616  -  Elapsed: 2:19:31\n",
      "  Batch 4,050 / 6,907  -  Avg Batch Loss: 1.7864  -  Elapsed: 2:21:20\n",
      "  Batch 4,100 / 6,907  -  Avg Batch Loss: 1.7724  -  Elapsed: 2:23:02\n",
      "  Batch 4,150 / 6,907  -  Avg Batch Loss: 1.7540  -  Elapsed: 2:24:43\n",
      "  Batch 4,200 / 6,907  -  Avg Batch Loss: 1.8163  -  Elapsed: 2:26:23\n",
      "  Batch 4,250 / 6,907  -  Avg Batch Loss: 1.7786  -  Elapsed: 2:28:04\n",
      "  Batch 4,300 / 6,907  -  Avg Batch Loss: 1.8124  -  Elapsed: 2:29:45\n",
      "  Batch 4,350 / 6,907  -  Avg Batch Loss: 1.7547  -  Elapsed: 2:31:26\n",
      "  Batch 4,400 / 6,907  -  Avg Batch Loss: 1.7042  -  Elapsed: 2:33:07\n",
      "  Batch 4,450 / 6,907  -  Avg Batch Loss: 1.8188  -  Elapsed: 2:34:47\n",
      "  Batch 4,500 / 6,907  -  Avg Batch Loss: 1.8001  -  Elapsed: 2:36:27\n",
      "  Batch 4,550 / 6,907  -  Avg Batch Loss: 1.7486  -  Elapsed: 2:38:08\n",
      "  Batch 4,600 / 6,907  -  Avg Batch Loss: 1.7606  -  Elapsed: 2:39:48\n",
      "  Batch 4,650 / 6,907  -  Avg Batch Loss: 1.7448  -  Elapsed: 2:41:29\n",
      "  Batch 4,700 / 6,907  -  Avg Batch Loss: 1.8202  -  Elapsed: 2:43:09\n",
      "  Batch 4,750 / 6,907  -  Avg Batch Loss: 1.8310  -  Elapsed: 2:44:49\n",
      "  Batch 4,800 / 6,907  -  Avg Batch Loss: 1.7894  -  Elapsed: 2:46:28\n",
      "  Batch 4,850 / 6,907  -  Avg Batch Loss: 1.7932  -  Elapsed: 2:48:08\n",
      "  Batch 4,900 / 6,907  -  Avg Batch Loss: 1.7605  -  Elapsed: 2:49:47\n",
      "  Batch 4,950 / 6,907  -  Avg Batch Loss: 1.7807  -  Elapsed: 2:51:26\n",
      "  Batch 5,000 / 6,907  -  Avg Batch Loss: 1.7849  -  Elapsed: 2:53:05\n",
      "  Batch 5,050 / 6,907  -  Avg Batch Loss: 1.7926  -  Elapsed: 2:54:44\n",
      "  Batch 5,100 / 6,907  -  Avg Batch Loss: 1.8069  -  Elapsed: 2:56:22\n",
      "  Batch 5,150 / 6,907  -  Avg Batch Loss: 1.7517  -  Elapsed: 2:58:01\n",
      "  Batch 5,200 / 6,907  -  Avg Batch Loss: 1.8181  -  Elapsed: 2:59:40\n",
      "  Batch 5,250 / 6,907  -  Avg Batch Loss: 1.7880  -  Elapsed: 3:01:19\n",
      "  Batch 5,300 / 6,907  -  Avg Batch Loss: 1.7432  -  Elapsed: 3:02:58\n",
      "  Batch 5,350 / 6,907  -  Avg Batch Loss: 1.7498  -  Elapsed: 3:04:37\n",
      "  Batch 5,400 / 6,907  -  Avg Batch Loss: 1.7585  -  Elapsed: 3:06:16\n",
      "  Batch 5,450 / 6,907  -  Avg Batch Loss: 1.7759  -  Elapsed: 3:07:54\n",
      "  Batch 5,500 / 6,907  -  Avg Batch Loss: 1.7810  -  Elapsed: 3:09:33\n",
      "  Batch 5,550 / 6,907  -  Avg Batch Loss: 1.7602  -  Elapsed: 3:11:12\n",
      "  Batch 5,600 / 6,907  -  Avg Batch Loss: 1.7929  -  Elapsed: 3:12:52\n",
      "  Batch 5,650 / 6,907  -  Avg Batch Loss: 1.7744  -  Elapsed: 3:14:31\n",
      "  Batch 5,700 / 6,907  -  Avg Batch Loss: 1.8059  -  Elapsed: 3:16:10\n",
      "  Batch 5,750 / 6,907  -  Avg Batch Loss: 1.7939  -  Elapsed: 3:17:50\n",
      "  Batch 5,800 / 6,907  -  Avg Batch Loss: 1.7850  -  Elapsed: 3:19:29\n",
      "  Batch 5,850 / 6,907  -  Avg Batch Loss: 1.7760  -  Elapsed: 3:21:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,900 / 6,907  -  Avg Batch Loss: 1.7957  -  Elapsed: 3:22:47\n",
      "  Batch 5,950 / 6,907  -  Avg Batch Loss: 1.7860  -  Elapsed: 3:24:26\n",
      "  Batch 6,000 / 6,907  -  Avg Batch Loss: 1.7630  -  Elapsed: 3:26:05\n",
      "  Batch 6,050 / 6,907  -  Avg Batch Loss: 1.7927  -  Elapsed: 3:27:44\n",
      "  Batch 6,100 / 6,907  -  Avg Batch Loss: 1.7905  -  Elapsed: 3:29:24\n",
      "  Batch 6,150 / 6,907  -  Avg Batch Loss: 1.7974  -  Elapsed: 3:31:03\n",
      "  Batch 6,200 / 6,907  -  Avg Batch Loss: 1.7777  -  Elapsed: 3:32:42\n",
      "  Batch 6,250 / 6,907  -  Avg Batch Loss: 1.7812  -  Elapsed: 3:34:21\n",
      "  Batch 6,300 / 6,907  -  Avg Batch Loss: 1.7998  -  Elapsed: 3:36:01\n",
      "  Batch 6,350 / 6,907  -  Avg Batch Loss: 1.7455  -  Elapsed: 3:37:40\n",
      "  Batch 6,400 / 6,907  -  Avg Batch Loss: 1.7718  -  Elapsed: 3:39:19\n",
      "  Batch 6,450 / 6,907  -  Avg Batch Loss: 1.7243  -  Elapsed: 3:40:58\n",
      "  Batch 6,500 / 6,907  -  Avg Batch Loss: 1.7932  -  Elapsed: 3:42:37\n",
      "  Batch 6,550 / 6,907  -  Avg Batch Loss: 1.7774  -  Elapsed: 3:44:17\n",
      "  Batch 6,600 / 6,907  -  Avg Batch Loss: 1.7831  -  Elapsed: 3:45:57\n",
      "  Batch 6,650 / 6,907  -  Avg Batch Loss: 1.7920  -  Elapsed: 3:47:36\n",
      "  Batch 6,700 / 6,907  -  Avg Batch Loss: 1.7750  -  Elapsed: 3:49:16\n",
      "  Batch 6,750 / 6,907  -  Avg Batch Loss: 1.7723  -  Elapsed: 3:50:56\n",
      "  Batch 6,800 / 6,907  -  Avg Batch Loss: 1.7998  -  Elapsed: 3:52:36\n",
      "  Batch 6,850 / 6,907  -  Avg Batch Loss: 1.7917  -  Elapsed: 3:54:15\n",
      "  Batch 6,900 / 6,907  -  Avg Batch Loss: 1.7773  -  Elapsed: 3:55:54\n",
      "\n",
      "  Average training loss: 1.79\n",
      "  Training epoch took: 3:56:08\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.20\n",
      "  Validation Loss: 1.78\n",
      "  Validation F1 Score: 0.198\n",
      "  Validation took: 0:06:27\n",
      "\n",
      "Training complete!\n",
      "Total training took 4:02:35 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "stats_bert = BERT_fine_tune_train(bert_cls_weighted_loss_mdl,\n",
    "                    train_dataloader,\n",
    "                    validation_dataloader,\n",
    "                    device = device,\n",
    "                    metric = metric,\n",
    "                    optimizer = optim,\n",
    "                    scheduler = scheduler,\n",
    "                    epochs = epochs,\n",
    "                    lr = lr,\n",
    "                    n_warmup = n_warmup\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6a5825b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(stats_bert[0]['val_stats']['y_true'], \n",
    "                      stats_bert[0]['val_stats']['y_pred'], \n",
    "                      idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a943e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c9fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95597c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "832fc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = iter(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ee4d5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[[   0, 3972, 1045,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   0,  100,  524,  ...,    1,    1,    1]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]]])},\n",
       " tensor([0, 1])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = torch.tensor([])\n",
    "y_pred = torch.tensor([])\n",
    "s = next(n)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8cd2cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ids = s[0]['input_ids'].squeeze(1).to(device)\n",
    "tok_type_ids = s[0]['token_type_ids'].squeeze(1).to(device)\n",
    "attn_mask = s[0]['attention_mask'].squeeze(1).to(device)\n",
    "labels = s[1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5e92596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_model.zero_grad()\n",
    "roberta_model.eval()\n",
    "outs = roberta_model(inp_ids, \n",
    "                  token_type_ids = tok_type_ids, \n",
    "                  attention_mask = attn_mask, \n",
    "                  labels = labels.long(),\n",
    "                    output_attentions = True,\n",
    "                    output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "15d6ac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.SequenceClassifierOutput"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0c05db2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [180]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mouts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "outs.last_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d25aa722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 768])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these should NOT be identical predictions \n",
    "outs['hidden_states'][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "89d76ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = abs(torch.randn(6)), abs(torch.randn(6))\n",
    "roberta_cls_weighted_loss_mdl.zero_grad()\n",
    "roberta_cls_weighted_loss_mdl.eval()\n",
    "outs = roberta_cls_weighted_loss_mdl(inp_ids, \n",
    "                  token_type_ids = tok_type_ids, \n",
    "                  attention_mask = attn_mask, \n",
    "                  labels = labels.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a0bc577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.6377, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2797,  0.0835, -0.1836, -0.1519,  0.1662, -0.2958],\n",
       "        [ 0.2797,  0.0835, -0.1836, -0.1519,  0.1662, -0.2958]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8679a95c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'onehot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [255]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monehot\u001b[49m(labels, \u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'onehot'"
     ]
    }
   ],
   "source": [
    "torch.onehot(labels, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b210640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unweighted_cel = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c47c9921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8289, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_cel(outs.logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "33d95fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = roberta_base(inp_ids, \n",
    "                  token_type_ids = tok_type_ids, \n",
    "                  attention_mask = attn_mask)\n",
    "                  #labels = labels.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fe7389e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0646,  0.0769, -0.0031,  ..., -0.0776, -0.0158, -0.0197],\n",
       "         [-0.0927, -0.0502,  0.0556,  ..., -0.0848,  0.0648,  0.0055],\n",
       "         [ 0.0831,  0.0728,  0.2277,  ..., -0.1048,  0.2363,  0.0517],\n",
       "         ...,\n",
       "         [-0.0154,  0.0519,  0.0381,  ..., -0.0384,  0.0453, -0.0053],\n",
       "         [ 0.0042,  0.0628,  0.0564,  ..., -0.0166,  0.0023, -0.0335],\n",
       "         [ 0.0391,  0.0482,  0.1157,  ..., -0.0126, -0.0041,  0.0547]],\n",
       "\n",
       "        [[-0.0662,  0.1161, -0.0198,  ..., -0.0506, -0.0339, -0.0117],\n",
       "         [-0.0704,  0.1625, -0.1756,  ...,  0.3305, -0.1445, -0.0913],\n",
       "         [ 0.1114,  0.0472,  0.0439,  ...,  0.1169, -0.0096, -0.0491],\n",
       "         ...,\n",
       "         [ 0.0590, -0.0644,  0.2094,  ...,  0.1910,  0.2377,  0.0746],\n",
       "         [ 0.0232,  0.0388,  0.0793,  ..., -0.0198,  0.0910,  0.1213],\n",
       "         [-0.0554,  0.0802, -0.0276,  ..., -0.0137, -0.0245,  0.0167]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "bc7b6772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6354e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0c06e38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3450ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BertClassifierCustom(\n",
       "  (model): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.15, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_cls_weighted_loss_mdl.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ab6c28ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b8da228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.15, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3adb443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "bert_base = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "bert_base.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6b1411c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1024) to match target batch_size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [148]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroberta_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtok_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1117\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1116\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1117\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1120\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1024) to match target batch_size (2)."
     ]
    }
   ],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model, num_labels): #checkpoint, num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = model# = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        #Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #Add custom layers\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "\n",
    "        logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "a0e2cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs.squeeze(),  targets.float())\n",
    "        loss = self.alpha * (1 - torch.exp(-ce_loss)) ** self.gamma * ce_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "815b33cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [738]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m focal_loss \u001b[38;5;241m=\u001b[39m FocalLoss(alpha\u001b[38;5;241m=\u001b[39mweights, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfocal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [734]\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[0;32m---> 13\u001b[0m     ce_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mce_loss)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m ce_loss\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/pyto/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "focal_loss = FocalLoss(alpha=weights, gamma=2)\n",
    "focal_loss(outs.logits, labels.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "10ec4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = torch.zeros((batch_size, n_classes))\n",
    "#focal_loss(outs.logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "1f147700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "F.one_hot(labels.long(), num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "86bf2db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "a65a61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outs.logits.detach().cpu()\n",
    "label_ids = labels.to('cpu').numpy()\n",
    "pred = logits.argmax(dim=1).unsqueeze(0)\n",
    "y_pred = torch.cat((y_pred, pred), -1)\n",
    "#label_ids = label_ids.cat(b_labels.to('cpu').numpy())\n",
    "y_true = torch.cat((y_true, labels.detach().cpu()), -1)\n",
    "#label_ids = torch.cat(label_ids, b_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f3080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyto_env",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
