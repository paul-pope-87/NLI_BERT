{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488c284c",
   "metadata": {},
   "source": [
    "<h1> Custom Parallel BERT </h1>\n",
    "\n",
    "Training a BERT classifier means many of the features are latent in the attention mechanism, and the classifier token neglects to fully represent the hard work that BERT does in gathering its representation. \n",
    "\n",
    "To make the model more transparent, I am building six attention modules (one for each L1 group) which are narrowed dimensionally by linear layers and then connected by a classifier. After training, the modules theoretically should be detachable: one mini-model for each L1 group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84045550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48236dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3e2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the submodule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af959f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parallel model\n",
    "\n",
    "for j in range(n_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyto_env",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
