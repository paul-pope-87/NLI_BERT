{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372fa2fa",
   "metadata": {},
   "source": [
    "<h1> BERT Visualization Techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566cdd6d",
   "metadata": {},
   "source": [
    "Aims\n",
    "\n",
    "1. Experiment with various visualization techniques and modules specific to BERT for PyTorch.\n",
    "2. Assess which methods suit the project best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01608cbe",
   "metadata": {},
   "source": [
    "Initial sources:\n",
    "    \n",
    "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1 \n",
    "    \n",
    "https://github.com/jessevig/bertviz#neuron-view  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c3ac3",
   "metadata": {},
   "source": [
    "<h2> Techniques to Try </h2>\n",
    "\n",
    "1. attention-head view https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization <br>\n",
    "2. model view <br>\n",
    "3. neuron view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0425f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipynb.fs.defs.BERT_Initial_Training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel, BertForSequenceClassification\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#import functions from other notebooks\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipynb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBERT_Initial_Training\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipynb.fs.defs.BERT_Initial_Training'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from bertviz import head_view, model_view\n",
    "from bertviz.neuron_view import show\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "#import functions from other notebooks\n",
    "from ipynb.fs.defs.BERT_Initial_Training import ClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0907cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/Users/paulp/Desktop/UEF/Thesis\"\n",
    "data_dir = os.path.join(project_dir, 'Data')\n",
    "model_dir = os.path.join(project_dir, 'Models/classifier')\n",
    "model_path = 'CLS_2ep_256_e-5_model.pt'\n",
    "test_set_path = 'test.csv'\n",
    "\n",
    "os.chdir(project_dir)\n",
    "test_set = pd.read_csv(os.path.join(data_dir, test_set_path))\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "spec_tokens = ['<?>', '<*>', '<R>', #one of the corpora uses these\n",
    "               '<MISC>', \n",
    "               '<LOC>', \n",
    "               '<PER>', \n",
    "               '<ORG>'] # these will mask named entities later if needed\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', \n",
    "                                          additional_special_tokens = spec_tokens,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class wrapped around torch.utils dataset\n",
    "\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [target_idx[a] for a in df['Target']]\n",
    "        self.texts = [tokenizer.encode_plus(text,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=512,\n",
    "                                truncation=True,\n",
    "                                padding='max_length',\n",
    "                                return_attention_mask = True,\n",
    "                                return_tensors='pt') \n",
    "                      for text in df.loc[:,'Text']]\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        \n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3aabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "ds_test = ClassificationDataset(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    ds_test,\n",
    "    sampler = SequentialSampler(ds_test),\n",
    "    batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target:idx dict\n",
    "target_idx = {b:a for a,b in enumerate(test_set['Target'].unique())}\n",
    "idx_target = {target_idx[a]:a for a in target_idx.keys()}\n",
    "# load model : uncomment if not already loaded\n",
    "#pretrained_state_dict = torch.load('pretrained_2ep_1.pt')\n",
    "classification_model = BertForSequenceClassification.from_pretrained(\n",
    "    model_dir, #model_dir,\n",
    "    num_labels = len(target_idx.keys()),\n",
    "    output_attentions = True,\n",
    "    output_hidden_states = False,) \n",
    "\n",
    "#classification_model.load_state_dict(\n",
    "#    torch.load(os.path.join(model_dir, 'classification_2ep.pt')),\n",
    "#    strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97493d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_set.sample(1)\n",
    "target = sample['Target'].item()\n",
    "inputs = tokenizer.encode_plus(sample['Text'].item(),\n",
    "          return_tensors='pt',\n",
    "          max_length=30, \n",
    "          truncation=True, \n",
    "          padding='max_length',\n",
    "          )\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "outs = classification_model(input_ids, token_type_ids=token_type_ids) #attention weights are returned last\n",
    "attention = outs[-1]\n",
    "\n",
    "input_id_list = input_ids[0].tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "pred = idx_target[outs.logits.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('target: ', target, ', ', 'prediction: ', pred, '\\n\\n logits: ', outs.logits, '\\n\\n text: ', tokenizer.convert_ids_to_tokens(input_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7921e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d06624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neuron view\n",
    "show(classification_model, \n",
    "     model_type='bert', \n",
    "     tokenizer=tokenizer, \n",
    "     sentence_a = sample['Text'].item(),\n",
    "     layer=2, \n",
    "     head=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84053088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['Text'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336f393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyto_env",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
