{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653b01c9",
   "metadata": {},
   "source": [
    "<h1> Token Masking and Data Cleaning Functions </h1>\n",
    "\n",
    "The following content contains a function which:\n",
    "\n",
    "1. Masks certain named entities across the corpus\n",
    "2. Reduces strange corpus-specific encoding characters and idiosyncracies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff4fe",
   "metadata": {},
   "source": [
    "<h2> Token Masking </h2>\n",
    "\n",
    "In the data, LOC and MISC tokens tend to be the most imbalanced. I create a general function to mask NE tokens with their CONLL03 tag, then iterate over the dataset, making the chosen replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ae4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import spacy \n",
    "import importlib\n",
    "import pickle\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5adb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a652050a844b5b2bb1c477bf027c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 11:55:00 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | spacy   |\n",
      "| mwt       | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "| ner       | conll03 |\n",
      "=======================\n",
      "\n",
      "2022-10-23 11:55:00 INFO: Use device: cpu\n",
      "2022-10-23 11:55:00 INFO: Loading: tokenize\n",
      "2022-10-23 11:55:00 INFO: Loading: mwt\n",
      "2022-10-23 11:55:00 INFO: Loading: pos\n",
      "2022-10-23 11:55:00 INFO: Loading: lemma\n",
      "2022-10-23 11:55:00 INFO: Loading: depparse\n",
      "2022-10-23 11:55:00 INFO: Loading: ner\n",
      "2022-10-23 11:55:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Data \n",
    "\n",
    "project_dir = \"/Users/paulp/Library/CloudStorage/OneDrive-UniversityofEasternFinland/UEF/Thesis\"\n",
    "data_dir = os.path.join(project_dir,\"Data\")\n",
    "model_dir = os.path.join(project_dir, \"Models\")\n",
    "\n",
    "os.chdir(data_dir)\n",
    "\n",
    "old_dataset = pd.read_csv('compiled_data_set.csv', index_col = 0)\n",
    "\n",
    "# special tokens\n",
    "with open('spec_tokens_ne.txt', 'rb') as file:\n",
    "    spec_tokens = pickle.load(file)\n",
    "    \n",
    "# tokenizer and NER parser\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', \n",
    "                                          additional_special_tokens = spec_tokens,\n",
    "                                         unk_token = '[UNK]')\n",
    "\n",
    "#processors = {'tokenize':'spacy','ner':'conll03'}\n",
    "#stanza_ner = stanza.Pipeline('en', processors=processors, package='ewt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "33b73a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mother-in-law']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"mother-in-law\")\n",
    "print([t.text for t in doc]) # ['mother', '-', 'in', '-', 'law']\n",
    "\n",
    "# Modify tokenizer infix patterns\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        #  Commented out regex that splits on hyphens between letters:\n",
    "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    ")\n",
    "\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp_spacy.tokenizer.infix_finditer = infix_re.finditer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b722fef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7226d68f21f94de3859f6d67726768ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 18:22:57 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | spacy    |\n",
      "| pos          | combined |\n",
      "| lemma        | combined |\n",
      "| depparse     | combined |\n",
      "| sentiment    | sstplus  |\n",
      "| constituency | wsj      |\n",
      "| ner          | conll03  |\n",
      "===========================\n",
      "\n",
      "2022-10-23 18:22:57 INFO: Use device: cpu\n",
      "2022-10-23 18:22:57 INFO: Loading: tokenize\n",
      "2022-10-23 18:22:57 INFO: Loading: pos\n",
      "2022-10-23 18:22:57 INFO: Loading: lemma\n",
      "2022-10-23 18:22:57 INFO: Loading: depparse\n",
      "2022-10-23 18:22:57 INFO: Loading: sentiment\n",
      "2022-10-23 18:22:57 INFO: Loading: constituency\n",
      "2022-10-23 18:22:58 INFO: Loading: ner\n",
      "2022-10-23 18:22:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='en', processors={'tokenize':'spacy','ner':'conll03'}, tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "47b531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_replace(text, filter_out = ['MISC', 'LOC']):\n",
    "    \n",
    "    doc = nlp_spacy(text)\n",
    "    d = [[c.text for c in b.__iter__()] for b in doc.sents]\n",
    "    \n",
    "    p = nlp_stanza.process(d)\n",
    "    p = p.to_dict()\n",
    "    \n",
    "    new_tokens = []\n",
    "    for sent in p:\n",
    "        for tok in sent:\n",
    "\n",
    "            if tok['ner'] == 'O' or tok['ner'][2:] not in filter_out:\n",
    "                new_tok = tok['text']\n",
    "                new_tokens.append(new_tok)\n",
    "            elif tok['ner'][0] in ['S', 'E']:\n",
    "                new_tok = '<' + tok['ner'][2:] + '>'\n",
    "                new_tokens.append(new_tok)\n",
    "            elif tok['ner'][0] in ['B', 'I']:\n",
    "                pass\n",
    "    \n",
    "    # a more professional and faster way to do this would be to put replacements in a json\n",
    "    # and compile and do the replacements in one function\n",
    "    t = bert_tokenizer.convert_tokens_to_string(new_tokens)\n",
    "    t = re.sub('< ([\\*R\\?]) >', '<\\g<1>>', t)    \n",
    "    t = re.sub(' ([\\.\\?!,:;])', '\\g<1>', t)\n",
    "    t = re.sub('(\\() ', '\\g<1>', t)\n",
    "    t = re.sub(' (\\))', '\\g<1>', t)\n",
    "    t = re.sub(\" & quot;\", ' \"', t)\n",
    "    t = re.sub(\"&quot;([\\., \\?!])\", '\"\\g<1>', t)\n",
    "    t = re.sub('\\xa0', '', t)\n",
    "    t = re.sub(' quot;', ' \"', t)\n",
    "    t = re.sub('quot; ', '\" ', t)\n",
    "    t = re.sub('quot;[\\.,!\\?]', '\" ', t)\n",
    "    t = re.sub(\" 'm \", \"'m \", t)\n",
    "    t = re.sub(\" n't \", \"n't \", t)\n",
    "    t = re.sub(\" 've \", \"'ve \", t)\n",
    "    t = re.sub(\" 's \", \"'s \", t)\n",
    "    t = re.sub(\" 'll \", \"'ll \", t)\n",
    "    t = re.sub(\" 'd \", \"'d \", t)\n",
    "    t = re.sub(\" {2,}\", \" \", t)\n",
    "    t = re.sub(\" s' \", \"s' \", t)\n",
    "    t = re.sub(\" n't([ \\.,:;])\", \"n't\\g<1>\", t)\n",
    "    t = re.sub(\"%%\", \"\", t)\n",
    "    t = re.sub('\\n{1,}\\t{1,}', '\\n\\n', t)\n",
    "    t = re.sub('\\n{1,}', '\\n\\n', t)\n",
    "    t = re.sub('([\\.!\\?])([A-Za-z])', '\\g<1> \\g<2>', t)\n",
    "    t = re.sub('\\x81@', ' ', t)\n",
    "    t = re.sub(' amp; ', ' and ', t)\n",
    "    t = re.sub('[\\.,:]([A-Za-z])', '\\. \\g<1>', t)\n",
    "    t = re.sub('([\\.!\\?]){4,}', '\\g<1>', t)\n",
    "    t = re.sub(\"''\", \"'\", t)\n",
    "    t = t.strip()\n",
    "    \n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "8d6eec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes forever.\n",
    "masked_dataset = old_dataset\n",
    "masked_dataset['Text'] = old_dataset['Text'].apply(lambda x : ne_replace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "600ea561",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_dataset.to_csv('masked_data_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b71ed",
   "metadata": {},
   "source": [
    "<h1> References </h1>\n",
    "\n",
    "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020. [pdf]\n",
    "\n",
    "spaCy: Industrial-strength Natural Language Processing in Python. (2020). https://spacy.io/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyto_env",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
