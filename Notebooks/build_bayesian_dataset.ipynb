{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876a29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5a4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/Users/paulp/Library/CloudStorage/OneDrive-UniversityofEasternFinland/UEF/Thesis'\n",
    "data_dir = os.path.join(project_dir, 'Data')\n",
    "model_dir = os.path.join(project_dir, 'Models')\n",
    "\n",
    "os.chdir(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained('custom_bert_tokenizer')\n",
    "\n",
    "os.chdir(data_dir)\n",
    "\n",
    "#train = pd.read_csv('train.csv', index_col = 0).reset_index(drop=True)\n",
    "#test = pd.read_csv('test.csv', index_col = 0).reset_index(drop=True)\n",
    "full_dataset = pd.read_csv('masked_data_set.csv', index_col=0).reset_index(drop=True)\n",
    "\n",
    "with open('target_idx.json')as file:\n",
    "    target_idx = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f72b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build frequency distribution\n",
    "def build_frequency_dist(dataframe, vocab):\n",
    "    freq_dist = {b:{'id':vocab[b], 'freq':0} for b in vocab}\n",
    "    \n",
    "    for row in dataframe['Text']:\n",
    "        tokens = tokenizer.tokenize(row)\n",
    "        for tok in tokens:\n",
    "            try:\n",
    "                freq_dist[tok]['freq'] += 1\n",
    "            except:\n",
    "                pass\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4eebdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "freq_dist = build_frequency_dist(full_dataset, tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7419d9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gmail': {'id': 19841, 'freq': 6},\n",
       " 'invad': {'id': 16834, 'freq': 0},\n",
       " 'cash': {'id': 3992, 'freq': 199},\n",
       " '##guide': {'id': 15666, 'freq': 4},\n",
       " '##unt': {'id': 5512, 'freq': 23},\n",
       " 'disk': {'id': 17853, 'freq': 13},\n",
       " 'broken': {'id': 4165, 'freq': 183},\n",
       " 'worked': {'id': 2471, 'freq': 480},\n",
       " 'joeran': {'id': 20588, 'freq': 6},\n",
       " 'wearing': {'id': 3162, 'freq': 306},\n",
       " 'csr': {'id': 3337, 'freq': 276},\n",
       " '##uations': {'id': 12823, 'freq': 16},\n",
       " '##eer': {'id': 21628, 'freq': 18},\n",
       " '##j': {'id': 124, 'freq': 54},\n",
       " 'hateful': {'id': 19377, 'freq': 7},\n",
       " 'ranged': {'id': 18620, 'freq': 7},\n",
       " 'regime': {'id': 9375, 'freq': 36},\n",
       " 'divers': {'id': 4392, 'freq': 18},\n",
       " 'costomer': {'id': 20447, 'freq': 6},\n",
       " 'gabriela': {'id': 17379, 'freq': 9},\n",
       " 'prise': {'id': 22021, 'freq': 6},\n",
       " 'kil': {'id': 7569, 'freq': 13},\n",
       " 'loyalty': {'id': 10765, 'freq': 25},\n",
       " 'speaking': {'id': 2090, 'freq': 632},\n",
       " 'expressed': {'id': 7396, 'freq': 56},\n",
       " 'exit': {'id': 8912, 'freq': 38},\n",
       " 'blankets': {'id': 13133, 'freq': 16},\n",
       " '##osing': {'id': 14197, 'freq': 12},\n",
       " 'polluting': {'id': 7595, 'freq': 53},\n",
       " 'dependent': {'id': 6404, 'freq': 80},\n",
       " 'bare': {'id': 6883, 'freq': 30},\n",
       " 'arguement': {'id': 10166, 'freq': 28},\n",
       " 'grant': {'id': 7656, 'freq': 47},\n",
       " 'pyramids': {'id': 13108, 'freq': 16},\n",
       " 'h': {'id': 57, 'freq': 76},\n",
       " 'possession': {'id': 11578, 'freq': 21},\n",
       " '##ip': {'id': 551, 'freq': 52},\n",
       " '##ik': {'id': 3910, 'freq': 31},\n",
       " 'saf': {'id': 2685, 'freq': 18},\n",
       " 'productive': {'id': 6719, 'freq': 70},\n",
       " '##orow': {'id': 13237, 'freq': 0},\n",
       " 'respects': {'id': 7975, 'freq': 48},\n",
       " '##ices': {'id': 1171, 'freq': 17},\n",
       " '##ilings': {'id': 9936, 'freq': 0},\n",
       " 'refuted': {'id': 13791, 'freq': 14},\n",
       " 'terrified': {'id': 13946, 'freq': 14},\n",
       " '##arazz': {'id': 14624, 'freq': 0},\n",
       " '##kov': {'id': 15230, 'freq': 27},\n",
       " 'essentials': {'id': 15716, 'freq': 11},\n",
       " 'similar': {'id': 1904, 'freq': 520},\n",
       " 'presently': {'id': 15424, 'freq': 11},\n",
       " 'fear': {'id': 2920, 'freq': 279},\n",
       " 'monopol': {'id': 15742, 'freq': 11},\n",
       " 'bankrupt': {'id': 7332, 'freq': 45},\n",
       " 'pong': {'id': 16593, 'freq': 9},\n",
       " 'comed': {'id': 8910, 'freq': 2},\n",
       " 'twain': {'id': 20314, 'freq': 7},\n",
       " 'mediator': {'id': 21180, 'freq': 6},\n",
       " 'flash': {'id': 8937, 'freq': 32},\n",
       " '##entions': {'id': 5711, 'freq': 5},\n",
       " 'incorporate': {'id': 8505, 'freq': 46},\n",
       " 'thirties': {'id': 15133, 'freq': 12},\n",
       " '##emics': {'id': 18860, 'freq': 0},\n",
       " 'nutr': {'id': 19381, 'freq': 5},\n",
       " 'outstanding': {'id': 5530, 'freq': 105},\n",
       " 'consistency': {'id': 20788, 'freq': 6},\n",
       " 'ecperience': {'id': 22330, 'freq': 6},\n",
       " 'zh': {'id': 8897, 'freq': 9},\n",
       " '##uten': {'id': 12532, 'freq': 4},\n",
       " '##mall': {'id': 21645, 'freq': 5},\n",
       " 'compreh': {'id': 5548, 'freq': 6},\n",
       " 'implements': {'id': 18838, 'freq': 7},\n",
       " 'gas': {'id': 2380, 'freq': 382},\n",
       " 'deadline': {'id': 6699, 'freq': 70},\n",
       " 'gloves': {'id': 17938, 'freq': 8},\n",
       " 'recic': {'id': 22198, 'freq': 7},\n",
       " 'qualification': {'id': 6767, 'freq': 68},\n",
       " '##ill': {'id': 209, 'freq': 31},\n",
       " 'digging': {'id': 15323, 'freq': 11},\n",
       " '##isition': {'id': 7433, 'freq': 11},\n",
       " 'debated': {'id': 11763, 'freq': 20},\n",
       " 'reflection': {'id': 9534, 'freq': 34},\n",
       " 'faked': {'id': 16572, 'freq': 9},\n",
       " 'fully': {'id': 3493, 'freq': 256},\n",
       " '##ervation': {'id': 4804, 'freq': 1},\n",
       " 'fulf': {'id': 5546, 'freq': 1},\n",
       " '##educ': {'id': 11935, 'freq': 3},\n",
       " '##centage': {'id': 19247, 'freq': 5},\n",
       " 'cups': {'id': 10005, 'freq': 29},\n",
       " 'inventions': {'id': 3673, 'freq': 230},\n",
       " 'bleeding': {'id': 7613, 'freq': 53},\n",
       " '##erge': {'id': 10621, 'freq': 5},\n",
       " '##atisf': {'id': 5509, 'freq': 2},\n",
       " 'provincial': {'id': 16536, 'freq': 10},\n",
       " 'pollution': {'id': 1816, 'freq': 790},\n",
       " 'stamps': {'id': 19638, 'freq': 7},\n",
       " 'devise': {'id': 20633, 'freq': 13},\n",
       " '##orous': {'id': 6122, 'freq': 12},\n",
       " 'workshops': {'id': 12421, 'freq': 18},\n",
       " 'sooner': {'id': 6403, 'freq': 77},\n",
       " 'discounts': {'id': 9189, 'freq': 35},\n",
       " 'ille': {'id': 17084, 'freq': 4},\n",
       " 'harrold': {'id': 11569, 'freq': 1},\n",
       " 'silly': {'id': 7703, 'freq': 51},\n",
       " 'cheeks': {'id': 11635, 'freq': 21},\n",
       " '49': {'id': 16555, 'freq': 9},\n",
       " 'elizabeth': {'id': 5008, 'freq': 128},\n",
       " 'concert': {'id': 4697, 'freq': 102},\n",
       " 'interessting': {'id': 14478, 'freq': 13},\n",
       " '##falls': {'id': 15013, 'freq': 12},\n",
       " '##ando': {'id': 13677, 'freq': 4},\n",
       " 'correspond': {'id': 6978, 'freq': 27},\n",
       " 'reli': {'id': 8447, 'freq': 20},\n",
       " 'frustrated': {'id': 6611, 'freq': 72},\n",
       " 'rotfl': {'id': 21071, 'freq': 6},\n",
       " 'face': {'id': 1373, 'freq': 1248},\n",
       " 'tourism': {'id': 4234, 'freq': 178},\n",
       " 'skipping': {'id': 13895, 'freq': 14},\n",
       " 'credentials': {'id': 17417, 'freq': 9},\n",
       " 'der': {'id': 5896, 'freq': 33},\n",
       " 'sarina': {'id': 14368, 'freq': 13},\n",
       " 'advancements': {'id': 15570, 'freq': 11},\n",
       " 'jus': {'id': 18578, 'freq': 8},\n",
       " '##berger': {'id': 16825, 'freq': 7},\n",
       " 'yelled': {'id': 18644, 'freq': 7},\n",
       " 'practicable': {'id': 20580, 'freq': 6},\n",
       " 'therapy': {'id': 10991, 'freq': 24},\n",
       " 'politicians': {'id': 4516, 'freq': 157},\n",
       " 'spark': {'id': 7098, 'freq': 28},\n",
       " 'unforgetable': {'id': 13932, 'freq': 14},\n",
       " '##omer': {'id': 11223, 'freq': 9},\n",
       " 'venge': {'id': 21588, 'freq': 5},\n",
       " 'concering': {'id': 17961, 'freq': 8},\n",
       " 'glasses': {'id': 5727, 'freq': 99},\n",
       " 'verry': {'id': 15618, 'freq': 11},\n",
       " 'joyfull': {'id': 20776, 'freq': 6},\n",
       " 'eligibility': {'id': 13870, 'freq': 14},\n",
       " 'beasts': {'id': 17616, 'freq': 8},\n",
       " '##cribe': {'id': 17180, 'freq': 5},\n",
       " 'curb': {'id': 19109, 'freq': 9},\n",
       " 'generall': {'id': 20433, 'freq': 6},\n",
       " 'mell': {'id': 21944, 'freq': 6},\n",
       " 'bathtub': {'id': 16522, 'freq': 13},\n",
       " '##cion': {'id': 12515, 'freq': 27},\n",
       " 'contemporaries': {'id': 14510, 'freq': 13},\n",
       " 'mete': {'id': 9494, 'freq': 0},\n",
       " 'partys': {'id': 15369, 'freq': 11},\n",
       " 'practitioner': {'id': 19125, 'freq': 9},\n",
       " 'emi': {'id': 20544, 'freq': 17},\n",
       " 'transportational': {'id': 22491, 'freq': 5},\n",
       " 'lu': {'id': 1261, 'freq': 27},\n",
       " '##uries': {'id': 3523, 'freq': 4},\n",
       " 'buddy': {'id': 17077, 'freq': 9},\n",
       " 'thirds': {'id': 16958, 'freq': 9},\n",
       " '##z': {'id': 119, 'freq': 230},\n",
       " '##quis': {'id': 6584, 'freq': 5},\n",
       " 'je': {'id': 3895, 'freq': 24},\n",
       " 'also': {'id': 370, 'freq': 11210},\n",
       " '##ats': {'id': 3033, 'freq': 28},\n",
       " 'cigaret': {'id': 4367, 'freq': 6},\n",
       " 'eld': {'id': 1998, 'freq': 7},\n",
       " 'partnership': {'id': 10543, 'freq': 29},\n",
       " 'much': {'id': 480, 'freq': 6981},\n",
       " 'sizes': {'id': 11908, 'freq': 19},\n",
       " '##rent': {'id': 1634, 'freq': 31},\n",
       " 'crowded': {'id': 5803, 'freq': 96},\n",
       " 'explode': {'id': 18230, 'freq': 10},\n",
       " 'strikes': {'id': 8672, 'freq': 40},\n",
       " 'pairs': {'id': 18614, 'freq': 7},\n",
       " 'depend': {'id': 1753, 'freq': 389},\n",
       " 'jasm': {'id': 21522, 'freq': 5},\n",
       " '##aru': {'id': 12524, 'freq': 7},\n",
       " 'assass': {'id': 14263, 'freq': 13},\n",
       " '##izz': {'id': 5216, 'freq': 9},\n",
       " 'operators': {'id': 18096, 'freq': 8},\n",
       " 'beh': {'id': 1184, 'freq': 20},\n",
       " 'ju': {'id': 2246, 'freq': 11},\n",
       " 'coin': {'id': 6300, 'freq': 51},\n",
       " 'cong': {'id': 16725, 'freq': 9},\n",
       " 'interrupt': {'id': 4233, 'freq': 33},\n",
       " 'colony': {'id': 13729, 'freq': 14},\n",
       " 'commu': {'id': 12885, 'freq': 16},\n",
       " 'opport': {'id': 1107, 'freq': 10},\n",
       " '##ton': {'id': 4916, 'freq': 45},\n",
       " '##tend': {'id': 1529, 'freq': 3},\n",
       " 'chemical': {'id': 5696, 'freq': 101},\n",
       " 'governor': {'id': 13744, 'freq': 18},\n",
       " '##due': {'id': 20210, 'freq': 8},\n",
       " '##anagh': {'id': 20897, 'freq': 0},\n",
       " 'severely': {'id': 9856, 'freq': 30},\n",
       " 'habit': {'id': 3188, 'freq': 237},\n",
       " 'handles': {'id': 22259, 'freq': 5},\n",
       " 'rogers': {'id': 19658, 'freq': 7},\n",
       " 'decent': {'id': 7099, 'freq': 62},\n",
       " 'alphab': {'id': 13556, 'freq': 0},\n",
       " 'dizzy': {'id': 20217, 'freq': 6},\n",
       " 'reflected': {'id': 7013, 'freq': 63},\n",
       " 'searched': {'id': 9161, 'freq': 35},\n",
       " 'clear': {'id': 1297, 'freq': 972},\n",
       " 'impressions': {'id': 11328, 'freq': 24},\n",
       " 'adults': {'id': 2693, 'freq': 418},\n",
       " 'pizz': {'id': 5810, 'freq': 1},\n",
       " 'thoug': {'id': 9608, 'freq': 7},\n",
       " 'irritated': {'id': 11176, 'freq': 23},\n",
       " '2004': {'id': 16231, 'freq': 10},\n",
       " 'priority': {'id': 5888, 'freq': 93},\n",
       " 'gal': {'id': 9447, 'freq': 24},\n",
       " 'spir': {'id': 3380, 'freq': 1},\n",
       " 'musical': {'id': 8292, 'freq': 46},\n",
       " 'mont': {'id': 15429, 'freq': 24},\n",
       " 'involve': {'id': 4385, 'freq': 145},\n",
       " 'du': {'id': 5868, 'freq': 28},\n",
       " 'abortions': {'id': 6381, 'freq': 78},\n",
       " '3d': {'id': 14007, 'freq': 16},\n",
       " 'jail': {'id': 4750, 'freq': 129},\n",
       " 'stro': {'id': 5647, 'freq': 22},\n",
       " 'succeed': {'id': 2376, 'freq': 398},\n",
       " 'luggage': {'id': 11527, 'freq': 21},\n",
       " 'aug': {'id': 5462, 'freq': 18},\n",
       " 'ought': {'id': 5412, 'freq': 110},\n",
       " '##ect': {'id': 394, 'freq': 49},\n",
       " 'journalist': {'id': 8875, 'freq': 38},\n",
       " '40': {'id': 4749, 'freq': 141},\n",
       " 'wednesday': {'id': 6250, 'freq': 86},\n",
       " '##ause': {'id': 319, 'freq': 18},\n",
       " '##ify': {'id': 2622, 'freq': 46},\n",
       " 'mum': {'id': 6455, 'freq': 66},\n",
       " 'prodcuts': {'id': 14429, 'freq': 13},\n",
       " 'hart': {'id': 15846, 'freq': 10},\n",
       " 'planting': {'id': 13068, 'freq': 16},\n",
       " 'anyone': {'id': 3013, 'freq': 337},\n",
       " 'vulgar': {'id': 17204, 'freq': 10},\n",
       " 'galileo': {'id': 18484, 'freq': 8},\n",
       " 'frowned': {'id': 21918, 'freq': 5},\n",
       " '##lling': {'id': 1333, 'freq': 6},\n",
       " 'afford': {'id': 2007, 'freq': 558},\n",
       " '##lifting': {'id': 7028, 'freq': 7},\n",
       " 'anywhere': {'id': 4262, 'freq': 176},\n",
       " 'repeating': {'id': 9540, 'freq': 32},\n",
       " 'confidence': {'id': 3202, 'freq': 301},\n",
       " 'flo': {'id': 1977, 'freq': 14},\n",
       " 'christ': {'id': 4402, 'freq': 32},\n",
       " 'viewing': {'id': 8100, 'freq': 46},\n",
       " 'pan': {'id': 2812, 'freq': 41},\n",
       " 'phoned': {'id': 14216, 'freq': 13},\n",
       " 'everz': {'id': 16780, 'freq': 10},\n",
       " 'enjoyments': {'id': 18916, 'freq': 7},\n",
       " 'holmes': {'id': 20713, 'freq': 6},\n",
       " 'rythm': {'id': 14459, 'freq': 14},\n",
       " '##orrow': {'id': 1984, 'freq': 5},\n",
       " 'kik': {'id': 15190, 'freq': 4},\n",
       " 'kept': {'id': 3775, 'freq': 220},\n",
       " '##other': {'id': 558, 'freq': 14},\n",
       " 'facil': {'id': 2570, 'freq': 16},\n",
       " 'mysterious': {'id': 7772, 'freq': 51},\n",
       " 'squee': {'id': 12769, 'freq': 4},\n",
       " 'accommodate': {'id': 15055, 'freq': 13},\n",
       " '##taic': {'id': 19451, 'freq': 1},\n",
       " 'harroldson': {'id': 11884, 'freq': 20},\n",
       " 'abrupt': {'id': 14674, 'freq': 6},\n",
       " 'outloo': {'id': 20275, 'freq': 0},\n",
       " 'gluc': {'id': 20498, 'freq': 1},\n",
       " 'sells': {'id': 7110, 'freq': 61},\n",
       " 've': {'id': 1177, 'freq': 1133},\n",
       " 'registration': {'id': 10594, 'freq': 27},\n",
       " 'utmost': {'id': 10737, 'freq': 25},\n",
       " 'businessman': {'id': 7051, 'freq': 62},\n",
       " 'collapse': {'id': 8578, 'freq': 42},\n",
       " 'gains': {'id': 10150, 'freq': 28},\n",
       " 'shure': {'id': 14660, 'freq': 12},\n",
       " 'hugging': {'id': 20539, 'freq': 6},\n",
       " 'practising': {'id': 14272, 'freq': 13},\n",
       " 'practicality': {'id': 19276, 'freq': 7},\n",
       " 'iphone': {'id': 21515, 'freq': 6},\n",
       " 'dispoint': {'id': 22216, 'freq': 5},\n",
       " 'even': {'id': 487, 'freq': 6145},\n",
       " 'compromise': {'id': 7354, 'freq': 57},\n",
       " '##arin': {'id': 20039, 'freq': 11},\n",
       " '##esses': {'id': 14663, 'freq': 11},\n",
       " 'plaz': {'id': 18849, 'freq': 8},\n",
       " 'recomended': {'id': 21093, 'freq': 6},\n",
       " 'professors': {'id': 5370, 'freq': 111},\n",
       " 'xi': {'id': 21597, 'freq': 16},\n",
       " '##otes': {'id': 5993, 'freq': 5},\n",
       " 'graphic': {'id': 11082, 'freq': 29},\n",
       " 'requirement': {'id': 5260, 'freq': 116},\n",
       " 'unne': {'id': 5046, 'freq': 18},\n",
       " 'settle': {'id': 5597, 'freq': 89},\n",
       " 'noisy': {'id': 7310, 'freq': 57},\n",
       " 'electronics': {'id': 12756, 'freq': 17},\n",
       " 'cardholders': {'id': 12929, 'freq': 16},\n",
       " 'columbus': {'id': 18491, 'freq': 8},\n",
       " 'pieces': {'id': 5545, 'freq': 105},\n",
       " 'discribed': {'id': 19016, 'freq': 7},\n",
       " 'cleverly': {'id': 18235, 'freq': 8},\n",
       " 'reas': {'id': 553, 'freq': 29},\n",
       " '##cel': {'id': 5528, 'freq': 11},\n",
       " 'sense': {'id': 1874, 'freq': 754},\n",
       " 'stake': {'id': 8832, 'freq': 18},\n",
       " 'evolution': {'id': 5902, 'freq': 93},\n",
       " 'nonbio': {'id': 9087, 'freq': 5},\n",
       " 'knees': {'id': 12554, 'freq': 17},\n",
       " '##rams': {'id': 21615, 'freq': 0},\n",
       " 'prepar': {'id': 4471, 'freq': 15},\n",
       " 'everywhere': {'id': 3012, 'freq': 337},\n",
       " 'packet': {'id': 12055, 'freq': 23},\n",
       " '##ani': {'id': 13235, 'freq': 20},\n",
       " 'achivements': {'id': 20979, 'freq': 6},\n",
       " 'catcher': {'id': 19279, 'freq': 7},\n",
       " '##gration': {'id': 16890, 'freq': 9},\n",
       " 'generalist': {'id': 9145, 'freq': 37},\n",
       " 'capacity': {'id': 4113, 'freq': 188},\n",
       " 'adverse': {'id': 4772, 'freq': 115},\n",
       " 'cards': {'id': 1228, 'freq': 1515},\n",
       " 'dynamics': {'id': 16364, 'freq': 10},\n",
       " 'compact': {'id': 12249, 'freq': 18},\n",
       " 'adop': {'id': 3712, 'freq': 2},\n",
       " 'wicked': {'id': 16613, 'freq': 10},\n",
       " 'exited': {'id': 14662, 'freq': 12},\n",
       " 'portfol': {'id': 20924, 'freq': 6},\n",
       " 'gass': {'id': 15182, 'freq': 14},\n",
       " 'alm': {'id': 16742, 'freq': 11},\n",
       " 'outside': {'id': 2042, 'freq': 651},\n",
       " 'cries': {'id': 16056, 'freq': 10},\n",
       " 'churches': {'id': 7121, 'freq': 61},\n",
       " 'enviorement': {'id': 19767, 'freq': 8},\n",
       " 'tins': {'id': 19914, 'freq': 7},\n",
       " '##ective': {'id': 20242, 'freq': 7},\n",
       " 'backgroung': {'id': 20977, 'freq': 6},\n",
       " 'alum': {'id': 12546, 'freq': 5},\n",
       " 'historically': {'id': 12660, 'freq': 17},\n",
       " 'continuity': {'id': 20775, 'freq': 6},\n",
       " 'whome': {'id': 21761, 'freq': 5},\n",
       " '##aties': {'id': 17584, 'freq': 3},\n",
       " 'aids': {'id': 10352, 'freq': 27},\n",
       " 'resistant': {'id': 14413, 'freq': 14},\n",
       " '##ber': {'id': 538, 'freq': 48},\n",
       " 'passively': {'id': 17957, 'freq': 8},\n",
       " 'nineteen': {'id': 7776, 'freq': 51},\n",
       " '##osse': {'id': 20345, 'freq': 7},\n",
       " 'osaka': {'id': 18342, 'freq': 8},\n",
       " 'passed': {'id': 3844, 'freq': 212},\n",
       " 'karl': {'id': 11372, 'freq': 25},\n",
       " 'together': {'id': 1281, 'freq': 1419},\n",
       " 'shoulder': {'id': 8028, 'freq': 49},\n",
       " 'azumi': {'id': 17295, 'freq': 9},\n",
       " 'carto': {'id': 6948, 'freq': 1},\n",
       " 'colder': {'id': 14352, 'freq': 13},\n",
       " '##usal': {'id': 14687, 'freq': 0},\n",
       " 'letter': {'id': 3488, 'freq': 257},\n",
       " 'establishment': {'id': 6239, 'freq': 82},\n",
       " 'unnot': {'id': 18862, 'freq': 1},\n",
       " 'pollutes': {'id': 9385, 'freq': 33},\n",
       " 'inject': {'id': 11932, 'freq': 10},\n",
       " 'uncontrollable': {'id': 18359, 'freq': 8},\n",
       " 'smile': {'id': 4817, 'freq': 138},\n",
       " '##essed': {'id': 11054, 'freq': 11},\n",
       " 'fairs': {'id': 19831, 'freq': 6},\n",
       " 'hoo': {'id': 12480, 'freq': 11},\n",
       " 'sage': {'id': 21575, 'freq': 6},\n",
       " 'rewrite': {'id': 15961, 'freq': 10},\n",
       " 'mosques': {'id': 18305, 'freq': 8},\n",
       " '2nd': {'id': 9728, 'freq': 30},\n",
       " 'deliveries': {'id': 20751, 'freq': 6},\n",
       " 'businessmen': {'id': 5905, 'freq': 92},\n",
       " 'ture': {'id': 8606, 'freq': 40},\n",
       " 'foul': {'id': 16571, 'freq': 13},\n",
       " 'explos': {'id': 7459, 'freq': 4},\n",
       " '##olation': {'id': 17735, 'freq': 6},\n",
       " 'univesity': {'id': 21379, 'freq': 6},\n",
       " 'activities': {'id': 1349, 'freq': 1325},\n",
       " 'themse': {'id': 16006, 'freq': 5},\n",
       " 'recong': {'id': 22199, 'freq': 5},\n",
       " 'infinite': {'id': 11728, 'freq': 20},\n",
       " 'pressure': {'id': 2360, 'freq': 520},\n",
       " 'yoga': {'id': 8598, 'freq': 41},\n",
       " '##illing': {'id': 6031, 'freq': 3},\n",
       " '##grad': {'id': 3922, 'freq': 1},\n",
       " 'cameras': {'id': 10202, 'freq': 28},\n",
       " 'concret': {'id': 20452, 'freq': 4},\n",
       " 'knowldge': {'id': 15091, 'freq': 12},\n",
       " 'terrace': {'id': 15774, 'freq': 16},\n",
       " 'pursued': {'id': 16299, 'freq': 10},\n",
       " 'chating': {'id': 14682, 'freq': 12},\n",
       " 'offending': {'id': 7610, 'freq': 53},\n",
       " 'question': {'id': 1218, 'freq': 1291},\n",
       " 'elig': {'id': 4613, 'freq': 9},\n",
       " '##acity': {'id': 14703, 'freq': 12},\n",
       " 'failed': {'id': 5298, 'freq': 114},\n",
       " 'shore': {'id': 9465, 'freq': 33},\n",
       " '##alf': {'id': 10450, 'freq': 1},\n",
       " 'injoy': {'id': 20773, 'freq': 7},\n",
       " 'tigers': {'id': 19772, 'freq': 7},\n",
       " 'basically': {'id': 4624, 'freq': 149},\n",
       " '##ping': {'id': 1694, 'freq': 55},\n",
       " '##igence': {'id': 12884, 'freq': 3},\n",
       " 'pist': {'id': 19884, 'freq': 7},\n",
       " '##orating': {'id': 21722, 'freq': 5},\n",
       " 'familys': {'id': 20419, 'freq': 6},\n",
       " 'politics': {'id': 1953, 'freq': 701},\n",
       " 'afer': {'id': 22098, 'freq': 6},\n",
       " 'enh': {'id': 3314, 'freq': 14},\n",
       " 'intrigued': {'id': 16395, 'freq': 10},\n",
       " 'cornerstone': {'id': 17135, 'freq': 9},\n",
       " 'if': {'id': 290, 'freq': 17988},\n",
       " 'cabin': {'id': 7417, 'freq': 16},\n",
       " 'insurance': {'id': 6174, 'freq': 88},\n",
       " 'mtv': {'id': 16587, 'freq': 9},\n",
       " 'colleages': {'id': 16951, 'freq': 9},\n",
       " 'nouwen': {'id': 18458, 'freq': 8},\n",
       " 'unfair': {'id': 4910, 'freq': 104},\n",
       " '##ream': {'id': 1079, 'freq': 3},\n",
       " '##stick': {'id': 18733, 'freq': 1},\n",
       " 'arguing': {'id': 6270, 'freq': 81},\n",
       " '##ervations': {'id': 7337, 'freq': 1},\n",
       " 'scor': {'id': 22423, 'freq': 12},\n",
       " 'unequ': {'id': 12071, 'freq': 7},\n",
       " 'forefront': {'id': 17366, 'freq': 9},\n",
       " '##ni': {'id': 21618, 'freq': 76},\n",
       " 'enogh': {'id': 11461, 'freq': 23},\n",
       " 'polic': {'id': 2378, 'freq': 6},\n",
       " 'extremes': {'id': 10718, 'freq': 25},\n",
       " 'scales': {'id': 7055, 'freq': 62},\n",
       " 'team': {'id': 1101, 'freq': 1538},\n",
       " 'rid': {'id': 3244, 'freq': 162},\n",
       " 'expans': {'id': 6228, 'freq': 0},\n",
       " '##asters': {'id': 5591, 'freq': 0},\n",
       " '##rue': {'id': 11031, 'freq': 13},\n",
       " '##icked': {'id': 12612, 'freq': 0},\n",
       " 'discoverd': {'id': 14868, 'freq': 12},\n",
       " 'voluntary': {'id': 6787, 'freq': 68},\n",
       " 'taxpay': {'id': 9674, 'freq': 0},\n",
       " '##ossed': {'id': 17029, 'freq': 5},\n",
       " 'output': {'id': 10107, 'freq': 28},\n",
       " 'handover': {'id': 20368, 'freq': 6},\n",
       " 'mpf': {'id': 4855, 'freq': 136},\n",
       " '##rupt': {'id': 2732, 'freq': 7},\n",
       " 'junk': {'id': 8117, 'freq': 48},\n",
       " '##ussed': {'id': 20521, 'freq': 5},\n",
       " '##ites': {'id': 2679, 'freq': 43},\n",
       " '##liness': {'id': 8158, 'freq': 7},\n",
       " 'hunger': {'id': 8392, 'freq': 45},\n",
       " 'partic': {'id': 1456, 'freq': 19},\n",
       " '##ace': {'id': 475, 'freq': 55},\n",
       " 'trips': {'id': 3587, 'freq': 243},\n",
       " 'foreign': {'id': 1619, 'freq': 717},\n",
       " 'either': {'id': 2183, 'freq': 591},\n",
       " 'som': {'id': 4144, 'freq': 38},\n",
       " 'onto': {'id': 7145, 'freq': 62},\n",
       " 'sweets': {'id': 8124, 'freq': 46},\n",
       " 'privilege': {'id': 12412, 'freq': 18},\n",
       " '##alling': {'id': 14672, 'freq': 5},\n",
       " 'attacking': {'id': 14769, 'freq': 12},\n",
       " 'contain': {'id': 4157, 'freq': 112},\n",
       " 'tat': {'id': 11202, 'freq': 18},\n",
       " 'mirrors': {'id': 13893, 'freq': 14},\n",
       " 'frame': {'id': 6759, 'freq': 45},\n",
       " 'barcel': {'id': 16216, 'freq': 0},\n",
       " 'haste': {'id': 22013, 'freq': 5},\n",
       " 'interisting': {'id': 22112, 'freq': 5},\n",
       " 'bianca': {'id': 19757, 'freq': 7},\n",
       " '##umptions': {'id': 12102, 'freq': 1},\n",
       " 'unconventional': {'id': 21049, 'freq': 6},\n",
       " 'steak': {'id': 12990, 'freq': 20},\n",
       " 'manipulate': {'id': 8765, 'freq': 39},\n",
       " 'problematic': {'id': 9364, 'freq': 36},\n",
       " 'atrocious': {'id': 18469, 'freq': 8},\n",
       " 'whisp': {'id': 15266, 'freq': 11},\n",
       " 'assessed': {'id': 13910, 'freq': 14},\n",
       " 'excellence': {'id': 11564, 'freq': 21},\n",
       " 'preventive': {'id': 19228, 'freq': 7},\n",
       " 'inactive': {'id': 20032, 'freq': 7},\n",
       " '##bend': {'id': 22391, 'freq': 2},\n",
       " 'infer': {'id': 8719, 'freq': 43},\n",
       " 'fash': {'id': 3114, 'freq': 9},\n",
       " 'phis': {'id': 10687, 'freq': 15},\n",
       " 'sad': {'id': 2678, 'freq': 276},\n",
       " '##itly': {'id': 6257, 'freq': 16},\n",
       " 'stunt': {'id': 10256, 'freq': 32},\n",
       " '##ions': {'id': 467, 'freq': 130},\n",
       " 'look': {'id': 872, 'freq': 2299},\n",
       " 'mid': {'id': 2140, 'freq': 75},\n",
       " 'experinces': {'id': 17041, 'freq': 9},\n",
       " 'barked': {'id': 19227, 'freq': 7},\n",
       " 'gorbach': {'id': 19764, 'freq': 7},\n",
       " 'interrupting': {'id': 14959, 'freq': 12},\n",
       " 'scient': {'id': 1915, 'freq': 20},\n",
       " 'n10': {'id': 19879, 'freq': 0},\n",
       " 'use': {'id': 438, 'freq': 6865},\n",
       " 'stare': {'id': 11679, 'freq': 20},\n",
       " '##astian': {'id': 10866, 'freq': 0},\n",
       " 'ener': {'id': 6226, 'freq': 2},\n",
       " '##stit': {'id': 9460, 'freq': 0},\n",
       " 'lier': {'id': 21823, 'freq': 8},\n",
       " 'oil': {'id': 1610, 'freq': 958},\n",
       " 'hello': {'id': 3006, 'freq': 340},\n",
       " 'prosper': {'id': 7416, 'freq': 21},\n",
       " 'demerit': {'id': 15074, 'freq': 12},\n",
       " '##sted': {'id': 14628, 'freq': 12},\n",
       " 'slogans': {'id': 5945, 'freq': 91},\n",
       " 'distination': {'id': 19138, 'freq': 7},\n",
       " 'spotlight': {'id': 14965, 'freq': 12},\n",
       " '##seller': {'id': 13254, 'freq': 0},\n",
       " 'legalizing': {'id': 5179, 'freq': 120},\n",
       " 'eid': {'id': 6157, 'freq': 87},\n",
       " 'worl': {'id': 15988, 'freq': 12},\n",
       " 'kenzyougo': {'id': 21078, 'freq': 6},\n",
       " 'paraly': {'id': 22487, 'freq': 7},\n",
       " 'mp': {'id': 4142, 'freq': 11},\n",
       " 'guest': {'id': 2837, 'freq': 117},\n",
       " 'meetings': {'id': 3233, 'freq': 295},\n",
       " 'enjoing': {'id': 10996, 'freq': 24},\n",
       " '##mpt': {'id': 2543, 'freq': 6},\n",
       " 'bias': {'id': 17006, 'freq': 12},\n",
       " '##aud': {'id': 6192, 'freq': 8},\n",
       " '##oul': {'id': 6462, 'freq': 13},\n",
       " '##olog': {'id': 707, 'freq': 7},\n",
       " 'nominating': {'id': 15779, 'freq': 11},\n",
       " 'syrian': {'id': 22352, 'freq': 5},\n",
       " 'lan': {'id': 10235, 'freq': 33},\n",
       " 'postp': {'id': 10177, 'freq': 6},\n",
       " 'fu': {'id': 1656, 'freq': 57},\n",
       " 'semest': {'id': 11593, 'freq': 2},\n",
       " 'spider': {'id': 22169, 'freq': 8},\n",
       " 'bussy': {'id': 22194, 'freq': 5},\n",
       " '##oth': {'id': 677, 'freq': 8},\n",
       " 'tricked': {'id': 12704, 'freq': 17},\n",
       " 'homosexuality': {'id': 19538, 'freq': 7},\n",
       " 'conveniences': {'id': 11304, 'freq': 22},\n",
       " 'advert': {'id': 576, 'freq': 40},\n",
       " 'term': {'id': 1396, 'freq': 679},\n",
       " 'confirmed': {'id': 9415, 'freq': 33},\n",
       " 'indulge': {'id': 11371, 'freq': 30},\n",
       " '##ason': {'id': 7200, 'freq': 6},\n",
       " '##fear': {'id': 18818, 'freq': 1},\n",
       " 'gil': {'id': 21496, 'freq': 13},\n",
       " '36': {'id': 7501, 'freq': 39},\n",
       " 'thant': {'id': 19994, 'freq': 6},\n",
       " 'demanding': {'id': 6015, 'freq': 89},\n",
       " 'subdiv': {'id': 14178, 'freq': 8},\n",
       " 'che': {'id': 1029, 'freq': 35},\n",
       " 'populations': {'id': 10136, 'freq': 28},\n",
       " 'togehter': {'id': 17412, 'freq': 9},\n",
       " 'spic': {'id': 22168, 'freq': 6},\n",
       " 'advise': {'id': 5109, 'freq': 124},\n",
       " '##rations': {'id': 9107, 'freq': 2},\n",
       " 'technic': {'id': 9797, 'freq': 25},\n",
       " '##akov': {'id': 17870, 'freq': 1},\n",
       " 'nervous': {'id': 5245, 'freq': 122},\n",
       " 'guarantees': {'id': 13873, 'freq': 14},\n",
       " 'whiter': {'id': 18731, 'freq': 7},\n",
       " '##onal': {'id': 5082, 'freq': 23},\n",
       " 'pab': {'id': 8006, 'freq': 1},\n",
       " 'inner': {'id': 5186, 'freq': 120},\n",
       " 'upsetting': {'id': 15647, 'freq': 11},\n",
       " 'fair': {'id': 2587, 'freq': 274},\n",
       " 'unseen': {'id': 18863, 'freq': 7},\n",
       " 'grav': {'id': 8176, 'freq': 5},\n",
       " 'offer': {'id': 1387, 'freq': 891},\n",
       " '##arters': {'id': 7447, 'freq': 0},\n",
       " '##ok': {'id': 14064, 'freq': 36},\n",
       " 'days': {'id': 1085, 'freq': 1863},\n",
       " '##common': {'id': 13907, 'freq': 0},\n",
       " 'gasp': {'id': 20703, 'freq': 10},\n",
       " 'bravely': {'id': 20931, 'freq': 6},\n",
       " 'teenages': {'id': 10516, 'freq': 26},\n",
       " 'crowding': {'id': 17092, 'freq': 9},\n",
       " '##ados': {'id': 15280, 'freq': 4},\n",
       " 'necessaries': {'id': 17947, 'freq': 8},\n",
       " 'subjetc': {'id': 21175, 'freq': 11},\n",
       " 'dorm': {'id': 10052, 'freq': 13},\n",
       " '##osity': {'id': 13711, 'freq': 2},\n",
       " 'irresponsibility': {'id': 18347, 'freq': 8},\n",
       " 'oportunity': {'id': 7774, 'freq': 51},\n",
       " 'choclate': {'id': 21357, 'freq': 9},\n",
       " 'personalities': {'id': 7215, 'freq': 59},\n",
       " 'fre': {'id': 1224, 'freq': 26},\n",
       " 'gain': {'id': 1758, 'freq': 597},\n",
       " '##ints': {'id': 5968, 'freq': 4},\n",
       " '##3': {'id': 135, 'freq': 105},\n",
       " 'bless': {'id': 6399, 'freq': 29},\n",
       " 'liv': {'id': 15970, 'freq': 14},\n",
       " 'sects': {'id': 15986, 'freq': 10},\n",
       " 'birthday': {'id': 2830, 'freq': 376},\n",
       " 'piss': {'id': 17484, 'freq': 8},\n",
       " 'yam': {'id': 12501, 'freq': 17},\n",
       " 'communists': {'id': 22119, 'freq': 5},\n",
       " 'metric': {'id': 17918, 'freq': 9},\n",
       " 'educate': {'id': 5593, 'freq': 106},\n",
       " 'laptops': {'id': 10751, 'freq': 25},\n",
       " 'princip': {'id': 3474, 'freq': 5},\n",
       " 'agencies': {'id': 6898, 'freq': 65},\n",
       " 'howling': {'id': 20221, 'freq': 6},\n",
       " 'struct': {'id': 2607, 'freq': 14},\n",
       " 'ignor': {'id': 5863, 'freq': 7},\n",
       " 'glimp': {'id': 16113, 'freq': 1},\n",
       " '##crafts': {'id': 18297, 'freq': 1},\n",
       " 'siemens': {'id': 21204, 'freq': 6},\n",
       " '##irm': {'id': 3951, 'freq': 10},\n",
       " '##oted': {'id': 3250, 'freq': 3},\n",
       " 'thomp': {'id': 16672, 'freq': 0},\n",
       " 'concent': {'id': 2389, 'freq': 13},\n",
       " 'intest': {'id': 20311, 'freq': 7},\n",
       " '##had': {'id': 16654, 'freq': 6},\n",
       " 'macho': {'id': 16199, 'freq': 10},\n",
       " 'goal': {'id': 1745, 'freq': 846},\n",
       " 'bags': {'id': 5919, 'freq': 92},\n",
       " 'codes': {'id': 16030, 'freq': 10},\n",
       " '##rap': {'id': 12824, 'freq': 9},\n",
       " '##chnology': {'id': 12765, 'freq': 16},\n",
       " 'tis': {'id': 19910, 'freq': 7},\n",
       " 'secretaries': {'id': 20849, 'freq': 6},\n",
       " 'lt': {'id': 10056, 'freq': 19},\n",
       " 'applies': {'id': 5904, 'freq': 93},\n",
       " 'obes': {'id': 9134, 'freq': 5},\n",
       " '##yr': {'id': 4130, 'freq': 10},\n",
       " 'offshore': {'id': 11401, 'freq': 23},\n",
       " 'tissue': {'id': 14493, 'freq': 13},\n",
       " '##acks': {'id': 2666, 'freq': 9},\n",
       " '##ise': {'id': 446, 'freq': 128},\n",
       " 'har': {'id': 3007, 'freq': 34},\n",
       " 'outgoing': {'id': 15354, 'freq': 13},\n",
       " 'knowledged': {'id': 22184, 'freq': 5},\n",
       " '##adow': {'id': 7797, 'freq': 1},\n",
       " 'addition': {'id': 1112, 'freq': 1386},\n",
       " 'appeared': {'id': 4146, 'freq': 185},\n",
       " 'daring': {'id': 11645, 'freq': 20},\n",
       " 'offence': {'id': 9640, 'freq': 33},\n",
       " 'produces': {'id': 5473, 'freq': 107},\n",
       " '##gest': {'id': 2554, 'freq': 1},\n",
       " 'variation': {'id': 12975, 'freq': 16},\n",
       " 'absolutely': {'id': 2806, 'freq': 382},\n",
       " 'thrilling': {'id': 13872, 'freq': 14},\n",
       " 'emit': {'id': 17966, 'freq': 18},\n",
       " '##igth': {'id': 11971, 'freq': 11},\n",
       " 'recognizing': {'id': 13419, 'freq': 15},\n",
       " 'feeds': {'id': 18112, 'freq': 8},\n",
       " '##rove': {'id': 6337, 'freq': 19},\n",
       " 'forens': {'id': 16925, 'freq': 0},\n",
       " 'inhale': {'id': 18318, 'freq': 9},\n",
       " 'hop': {'id': 3833, 'freq': 27},\n",
       " 'advent': {'id': 3527, 'freq': 26},\n",
       " 'norm': {'id': 1662, 'freq': 25},\n",
       " 'commite': {'id': 14716, 'freq': 4},\n",
       " 'felt': {'id': 2327, 'freq': 533},\n",
       " 'blast': {'id': 19039, 'freq': 11},\n",
       " 'uniquely': {'id': 19277, 'freq': 7},\n",
       " 'educ': {'id': 892, 'freq': 6},\n",
       " 'peace': {'id': 2916, 'freq': 209},\n",
       " 'poverty': {'id': 5027, 'freq': 127},\n",
       " 'executed': {'id': 9847, 'freq': 30},\n",
       " 'nose': {'id': 8359, 'freq': 45},\n",
       " 'disgust': {'id': 7779, 'freq': 6},\n",
       " 'chau': {'id': 8269, 'freq': 45},\n",
       " 'sellers': {'id': 8567, 'freq': 41},\n",
       " 'kle': {'id': 17463, 'freq': 9},\n",
       " '##made': {'id': 10432, 'freq': 3},\n",
       " '##asers': {'id': 15946, 'freq': 0},\n",
       " 'zara': {'id': 17511, 'freq': 8},\n",
       " 'researching': {'id': 8379, 'freq': 43},\n",
       " 'overth': {'id': 20373, 'freq': 8},\n",
       " 'tempered': {'id': 20732, 'freq': 6},\n",
       " 'as': {'id': 224, 'freq': 25863},\n",
       " '##cc': {'id': 336, 'freq': 38},\n",
       " '##roika': {'id': 21805, 'freq': 5},\n",
       " 'maike': {'id': 20164, 'freq': 6},\n",
       " '##pell': {'id': 21908, 'freq': 2},\n",
       " 'deter': {'id': 7464, 'freq': 17},\n",
       " '##atcher': {'id': 14405, 'freq': 6},\n",
       " 'cole': {'id': 20273, 'freq': 15},\n",
       " 'jones': {'id': 8243, 'freq': 44},\n",
       " 'nataly': {'id': 17933, 'freq': 0},\n",
       " 'booming': {'id': 12735, 'freq': 17},\n",
       " '##mt': {'id': 21642, 'freq': 20},\n",
       " 'scene': {'id': 4026, 'freq': 196},\n",
       " 'penny': {'id': 11789, 'freq': 20},\n",
       " 'snowy': {'id': 20908, 'freq': 6},\n",
       " 'regulating': {'id': 16294, 'freq': 10},\n",
       " 'embodies': {'id': 20794, 'freq': 6},\n",
       " 'nav': {'id': 6456, 'freq': 10},\n",
       " 'simultaneously': {'id': 8889, 'freq': 38},\n",
       " 'teas': {'id': 21980, 'freq': 11},\n",
       " 'robot': {'id': 1455, 'freq': 374},\n",
       " 'schu': {'id': 22105, 'freq': 11},\n",
       " 'exter': {'id': 11451, 'freq': 10},\n",
       " 'clothing': {'id': 6432, 'freq': 76},\n",
       " 'intoxication': {'id': 21293, 'freq': 6},\n",
       " 'biling': {'id': 11191, 'freq': 0},\n",
       " 'substit': {'id': 5183, 'freq': 9},\n",
       " '##holders': {'id': 5282, 'freq': 6},\n",
       " 'democracy': {'id': 7480, 'freq': 55},\n",
       " 'welcomed': {'id': 7699, 'freq': 52},\n",
       " 'camps': {'id': 14857, 'freq': 12},\n",
       " '##vents': {'id': 15235, 'freq': 2},\n",
       " 'enhancing': {'id': 10182, 'freq': 28},\n",
       " 'smiles': {'id': 10300, 'freq': 27},\n",
       " '##ivation': {'id': 16019, 'freq': 3},\n",
       " '##chem': {'id': 16704, 'freq': 3},\n",
       " 'thies': {'id': 17575, 'freq': 8},\n",
       " 'classical': {'id': 7249, 'freq': 58},\n",
       " 'az': {'id': 10230, 'freq': 9},\n",
       " 'packagemost': {'id': 19198, 'freq': 7},\n",
       " 'eras': {'id': 19329, 'freq': 9},\n",
       " '##ble': {'id': 278, 'freq': 171},\n",
       " 'curious': {'id': 5908, 'freq': 92},\n",
       " 'ocurred': {'id': 19529, 'freq': 7},\n",
       " 'endowed': {'id': 20479, 'freq': 6},\n",
       " 'healthy': {'id': 2004, 'freq': 675},\n",
       " '##viation': {'id': 15237, 'freq': 0},\n",
       " 'under': {'id': 471, 'freq': 957},\n",
       " 'tiles': {'id': 11656, 'freq': 20},\n",
       " '##cept': {'id': 560, 'freq': 8},\n",
       " 'icecream': {'id': 15662, 'freq': 11},\n",
       " '##hua': {'id': 19977, 'freq': 6},\n",
       " 'capitalistic': {'id': 13806, 'freq': 15},\n",
       " 'wat': {'id': 853, 'freq': 36},\n",
       " 'misunderstandings': {'id': 9566, 'freq': 32},\n",
       " 'passport': {'id': 7461, 'freq': 47},\n",
       " 'happ': {'id': 642, 'freq': 25},\n",
       " 'message': {'id': 3847, 'freq': 212},\n",
       " 'invest': {'id': 1508, 'freq': 275},\n",
       " 'leg': {'id': 1819, 'freq': 55},\n",
       " 'transmission': {'id': 11384, 'freq': 23},\n",
       " 'evac': {'id': 13305, 'freq': 6},\n",
       " 'toothpaste': {'id': 14001, 'freq': 15},\n",
       " 'interviewing': {'id': 17070, 'freq': 9},\n",
       " 'canal': {'id': 18775, 'freq': 7},\n",
       " 'pup': {'id': 3686, 'freq': 7},\n",
       " '##ersed': {'id': 14111, 'freq': 2},\n",
       " 'incidents': {'id': 8545, 'freq': 41},\n",
       " 'orgn': {'id': 14658, 'freq': 12},\n",
       " 'complet': {'id': 20533, 'freq': 12},\n",
       " '##oun': {'id': 2724, 'freq': 19},\n",
       " 'pronun': {'id': 7226, 'freq': 6},\n",
       " 'lois': {'id': 18763, 'freq': 7},\n",
       " 'statistically': {'id': 17013, 'freq': 9},\n",
       " 'forms': {'id': 3481, 'freq': 259},\n",
       " 'someone': {'id': 1172, 'freq': 1653},\n",
       " 'lege': {'id': 5784, 'freq': 1},\n",
       " 'relief': {'id': 6340, 'freq': 79},\n",
       " 'purchasing': {'id': 2213, 'freq': 582},\n",
       " 'systems': {'id': 3352, 'freq': 274},\n",
       " '##ires': {'id': 5681, 'freq': 6},\n",
       " 'unique': {'id': 3050, 'freq': 322},\n",
       " 'abol': {'id': 5339, 'freq': 3},\n",
       " 'theater': {'id': 8259, 'freq': 44},\n",
       " 'interp': {'id': 4831, 'freq': 5},\n",
       " 'needing': {'id': 10865, 'freq': 27},\n",
       " 'butcher': {'id': 12241, 'freq': 22},\n",
       " 'confide': {'id': 14815, 'freq': 17},\n",
       " 'nes': {'id': 9736, 'freq': 18},\n",
       " 'opinion': {'id': 802, 'freq': 2984},\n",
       " 'advance': {'id': 2931, 'freq': 298},\n",
       " 'terrific': {'id': 13406, 'freq': 15},\n",
       " 'herbal': {'id': 16821, 'freq': 9},\n",
       " '##quet': {'id': 8274, 'freq': 12},\n",
       " 'worthwhile': {'id': 9524, 'freq': 32},\n",
       " 'negotiated': {'id': 16356, 'freq': 10},\n",
       " '##zing': {'id': 6093, 'freq': 40},\n",
       " 'mr': {'id': 1657, 'freq': 714},\n",
       " 'economist': {'id': 14236, 'freq': 13},\n",
       " 'handmade': {'id': 22260, 'freq': 5},\n",
       " 'industrialisation': {'id': 6644, 'freq': 71},\n",
       " 'matching': {'id': 13852, 'freq': 14},\n",
       " 'knowladge': {'id': 16551, 'freq': 10},\n",
       " 'carre': {'id': 8540, 'freq': 6},\n",
       " 'demands': {'id': 5178, 'freq': 120},\n",
       " 'pizzas': {'id': 10971, 'freq': 24},\n",
       " 'arrogant': {'id': 11622, 'freq': 22},\n",
       " 'bred': {'id': 19809, 'freq': 6},\n",
       " 'reef': {'id': 21798, 'freq': 5},\n",
       " 'spl': {'id': 7244, 'freq': 6},\n",
       " 'nin': {'id': 10419, 'freq': 12},\n",
       " 'introduct': {'id': 18052, 'freq': 8},\n",
       " '##rob': {'id': 21803, 'freq': 9},\n",
       " 'outlet': {'id': 18897, 'freq': 7},\n",
       " 'intuition': {'id': 12443, 'freq': 19},\n",
       " 'computing': {'id': 10133, 'freq': 28},\n",
       " 'legendary': {'id': 10374, 'freq': 27},\n",
       " 'sel': {'id': 3428, 'freq': 23},\n",
       " 'probl': {'id': 12316, 'freq': 23},\n",
       " 'machines': {'id': 3760, 'freq': 222},\n",
       " '##aren': {'id': 7723, 'freq': 4},\n",
       " 'intertw': {'id': 13976, 'freq': 0},\n",
       " 'humid': {'id': 12932, 'freq': 16},\n",
       " 'gour': {'id': 15181, 'freq': 11},\n",
       " 'considering': {'id': 2861, 'freq': 370},\n",
       " 'transportation': {'id': 1883, 'freq': 742},\n",
       " 'through': {'id': 915, 'freq': 2070},\n",
       " 'con': {'id': 233, 'freq': 56},\n",
       " 'untou': {'id': 14847, 'freq': 3},\n",
       " 'crisp': {'id': 18195, 'freq': 10},\n",
       " '##cher': {'id': 8265, 'freq': 20},\n",
       " 'intermedi': {'id': 11485, 'freq': 3},\n",
       " 'liber': {'id': 4064, 'freq': 16},\n",
       " 'petro': {'id': 3289, 'freq': 8},\n",
       " 'parking': {'id': 4351, 'freq': 160},\n",
       " '##go': {'id': 3972, 'freq': 42},\n",
       " 'respondents': {'id': 6784, 'freq': 68},\n",
       " 'theire': {'id': 13231, 'freq': 15},\n",
       " '##iller': {'id': 15966, 'freq': 8},\n",
       " 'ax': {'id': 9729, 'freq': 9},\n",
       " 'utilization': {'id': 18277, 'freq': 8},\n",
       " 'inform': {'id': 725, 'freq': 161},\n",
       " 'equipment': {'id': 3761, 'freq': 222},\n",
       " 'ordinary': {'id': 3987, 'freq': 200},\n",
       " 'miser': {'id': 19232, 'freq': 9},\n",
       " 'somethink': {'id': 22149, 'freq': 5},\n",
       " 'contrasting': {'id': 13000, 'freq': 16},\n",
       " '##izers': {'id': 18875, 'freq': 5},\n",
       " 'exist': {'id': 1568, 'freq': 529},\n",
       " 'ink': {'id': 20026, 'freq': 6},\n",
       " 'mop': {'id': 18597, 'freq': 10},\n",
       " 'ank': {'id': 20119, 'freq': 9},\n",
       " 'limitations': {'id': 8663, 'freq': 40},\n",
       " 'innovative': {'id': 4670, 'freq': 148},\n",
       " 'suburban': {'id': 12722, 'freq': 17},\n",
       " 'oo': {'id': 21548, 'freq': 14},\n",
       " 'exceedingly': {'id': 19432, 'freq': 7},\n",
       " 'enrichment': {'id': 14989, 'freq': 12},\n",
       " 'creativ': {'id': 22439, 'freq': 6},\n",
       " 'strengthens': {'id': 20697, 'freq': 6},\n",
       " 'sedentary': {'id': 21132, 'freq': 6},\n",
       " 'infrastructure': {'id': 8138, 'freq': 46},\n",
       " 'ossis': {'id': 19770, 'freq': 7},\n",
       " 'raids': {'id': 13394, 'freq': 15},\n",
       " 'blames': {'id': 22372, 'freq': 5},\n",
       " 'comple': {'id': 1178, 'freq': 5},\n",
       " 'stagger': {'id': 15955, 'freq': 10},\n",
       " 'thats': {'id': 21759, 'freq': 6},\n",
       " 'numerous': {'id': 4438, 'freq': 165},\n",
       " 'donating': {'id': 6682, 'freq': 70},\n",
       " 'provider': {'id': 13745, 'freq': 14},\n",
       " 'marketers': {'id': 19058, 'freq': 7},\n",
       " 'wakes': {'id': 15212, 'freq': 11},\n",
       " 'compture': {'id': 21976, 'freq': 5},\n",
       " 'allegedly': {'id': 15048, 'freq': 12},\n",
       " '##atch': {'id': 5646, 'freq': 8},\n",
       " 'pose': {'id': 11468, 'freq': 26},\n",
       " 'imply': {'id': 7202, 'freq': 59},\n",
       " 'engineered': {'id': 11558, 'freq': 21},\n",
       " 'brotherhood': {'id': 19320, 'freq': 7},\n",
       " '##olution': {'id': 3562, 'freq': 2},\n",
       " '##ege': {'id': 15222, 'freq': 26},\n",
       " 'amelia': {'id': 12767, 'freq': 17},\n",
       " 'vacant': {'id': 16202, 'freq': 10},\n",
       " 'intersting': {'id': 10033, 'freq': 29},\n",
       " 'charisma': {'id': 5921, 'freq': 93},\n",
       " '##19': {'id': 15248, 'freq': 11},\n",
       " 'righteous': {'id': 21409, 'freq': 6},\n",
       " '##gang': {'id': 21624, 'freq': 6},\n",
       " 'cubed': {'id': 20825, 'freq': 6},\n",
       " '##bas': {'id': 19973, 'freq': 13},\n",
       " 'consideration': {'id': 2862, 'freq': 371},\n",
       " 'motivating': {'id': 7831, 'freq': 50},\n",
       " 'costing': {'id': 19029, 'freq': 7},\n",
       " 'discl': {'id': 11745, 'freq': 8},\n",
       " 'inward': {'id': 20031, 'freq': 9},\n",
       " 'chast': {'id': 21931, 'freq': 6},\n",
       " 'psychologically': {'id': 7348, 'freq': 57},\n",
       " '##eriences': {'id': 22082, 'freq': 0},\n",
       " 'happier': {'id': 5003, 'freq': 128},\n",
       " '##ating': {'id': 854, 'freq': 189},\n",
       " 'be': {'id': 170, 'freq': 35250},\n",
       " 'forie': {'id': 14093, 'freq': 0},\n",
       " 'rental': {'id': 10937, 'freq': 25},\n",
       " 'challen': {'id': 2073, 'freq': 0},\n",
       " '##cuts': {'id': 12209, 'freq': 4},\n",
       " 'backyard': {'id': 12941, 'freq': 18},\n",
       " 'jekyll': {'id': 14528, 'freq': 13},\n",
       " 'disrupted': {'id': 15053, 'freq': 12},\n",
       " 'bins': {'id': 15171, 'freq': 11},\n",
       " '##cated': {'id': 15915, 'freq': 4},\n",
       " 'foreseeable': {'id': 16433, 'freq': 10},\n",
       " '##vens': {'id': 21647, 'freq': 3},\n",
       " 'sting': {'id': 21781, 'freq': 7},\n",
       " 'repair': {'id': 6437, 'freq': 58},\n",
       " '##achy': {'id': 22080, 'freq': 5},\n",
       " 'gratifying': {'id': 17242, 'freq': 9},\n",
       " 'flex': {'id': 2375, 'freq': 17},\n",
       " '##essful': {'id': 17685, 'freq': 6},\n",
       " 'kno': {'id': 7883, 'freq': 6},\n",
       " '##okes': {'id': 13222, 'freq': 2},\n",
       " 'hasn': {'id': 5714, 'freq': 99},\n",
       " 'reserch': {'id': 13542, 'freq': 24},\n",
       " 'undes': {'id': 12079, 'freq': 9},\n",
       " 'sig': {'id': 19904, 'freq': 7},\n",
       " 'laden': {'id': 16295, 'freq': 10},\n",
       " 'translator': {'id': 13475, 'freq': 19},\n",
       " 'incorrect': {'id': 7181, 'freq': 52},\n",
       " 'ozon': {'id': 13554, 'freq': 15},\n",
       " 'frames': {'id': 15299, 'freq': 11},\n",
       " 'unde': {'id': 10941, 'freq': 21},\n",
       " '=': {'id': 40, 'freq': 31},\n",
       " '1960s': {'id': 10766, 'freq': 25},\n",
       " 'ruin': {'id': 7992, 'freq': 48},\n",
       " 'scenes': {'id': 9064, 'freq': 36},\n",
       " 'recruits': {'id': 14952, 'freq': 12},\n",
       " 'protein': {'id': 12759, 'freq': 19},\n",
       " '##hold': {'id': 3277, 'freq': 15},\n",
       " 'stucked': {'id': 18216, 'freq': 8},\n",
       " '5000': {'id': 11582, 'freq': 23},\n",
       " 'hacking': {'id': 10835, 'freq': 24},\n",
       " 'counselors': {'id': 13887, 'freq': 14},\n",
       " 'blushed': {'id': 20470, 'freq': 6},\n",
       " 'believer': {'id': 12998, 'freq': 16},\n",
       " 'fundraising': {'id': 18472, 'freq': 8},\n",
       " 'teal': {'id': 16003, 'freq': 10},\n",
       " 'unsk': {'id': 15589, 'freq': 1},\n",
       " 'ene': {'id': 13279, 'freq': 7},\n",
       " 'unpolite': {'id': 14958, 'freq': 13},\n",
       " 'indulged': {'id': 16383, 'freq': 10},\n",
       " 'venture': {'id': 17254, 'freq': 13},\n",
       " 'nj': {'id': 21538, 'freq': 5},\n",
       " '##worthy': {'id': 10147, 'freq': 8},\n",
       " 'childs': {'id': 12010, 'freq': 19},\n",
       " 'currently': {'id': 3578, 'freq': 245},\n",
       " 'having': {'id': 864, 'freq': 2685},\n",
       " 'atoms': {'id': 21942, 'freq': 6},\n",
       " 'internt': {'id': 22111, 'freq': 5},\n",
       " 'required': {'id': 2731, 'freq': 404},\n",
       " 'steam': {'id': 13403, 'freq': 20},\n",
       " 'deserved': {'id': 12016, 'freq': 19},\n",
       " '##othic': {'id': 16072, 'freq': 0},\n",
       " 'contib': {'id': 18925, 'freq': 7},\n",
       " 'vaccination': {'id': 19557, 'freq': 9},\n",
       " 'gren': {'id': 19837, 'freq': 7},\n",
       " 'babysitter': {'id': 21258, 'freq': 8},\n",
       " 'magiz': {'id': 22477, 'freq': 5},\n",
       " 'wating': {'id': 22334, 'freq': 5},\n",
       " 'club': {'id': 2615, 'freq': 282},\n",
       " 'workers': {'id': 1821, 'freq': 783},\n",
       " 'say': {'id': 637, 'freq': 3782},\n",
       " 'fragmented': {'id': 6494, 'freq': 75},\n",
       " 'scarce': {'id': 7694, 'freq': 43},\n",
       " 'egy': {'id': 15179, 'freq': 0},\n",
       " 'remodelled': {'id': 15778, 'freq': 11},\n",
       " 'intimate': {'id': 11724, 'freq': 22},\n",
       " 'organs': {'id': 9202, 'freq': 35},\n",
       " 'reason': {'id': 794, 'freq': 2764},\n",
       " 'feels': {'id': 4284, 'freq': 174},\n",
       " 'increse': {'id': 14777, 'freq': 14},\n",
       " 'abus': {'id': 21900, 'freq': 7},\n",
       " 'exagerated': {'id': 18380, 'freq': 8},\n",
       " 'nurture': {'id': 17116, 'freq': 17},\n",
       " '##ged': {'id': 3059, 'freq': 62},\n",
       " '##reed': {'id': 4645, 'freq': 3},\n",
       " 'convence': {'id': 17940, 'freq': 11},\n",
       " '##pper': {'id': 5273, 'freq': 4},\n",
       " 'hygi': {'id': 14323, 'freq': 1},\n",
       " 'scanned': {'id': 20513, 'freq': 6},\n",
       " 'join': {'id': 2526, 'freq': 391},\n",
       " '##iday': {'id': 2435, 'freq': 3},\n",
       " 'burglaries': {'id': 7931, 'freq': 49},\n",
       " 'hydrop': {'id': 9690, 'freq': 1},\n",
       " 'enables': {'id': 6298, 'freq': 80},\n",
       " 'diminish': {'id': 10775, 'freq': 25},\n",
       " 'laws': {'id': 2841, 'freq': 375},\n",
       " 'dram': {'id': 3676, 'freq': 2},\n",
       " 'dislikes': {'id': 13537, 'freq': 15},\n",
       " 'gender': {'id': 6776, 'freq': 68},\n",
       " 'diffic': {'id': 13301, 'freq': 15},\n",
       " 'preliminary': {'id': 15164, 'freq': 12},\n",
       " '##uum': {'id': 9334, 'freq': 2},\n",
       " 'ignores': {'id': 20784, 'freq': 6},\n",
       " 'unexpect': {'id': 20913, 'freq': 6},\n",
       " 'samuel': {'id': 17157, 'freq': 10},\n",
       " 'yearly': {'id': 16804, 'freq': 9},\n",
       " 'relation': {'id': 4378, 'freq': 166},\n",
       " 'misle': {'id': 6142, 'freq': 5},\n",
       " 'broad': {'id': 1022, 'freq': 1649},\n",
       " '##onsider': {'id': 8360, 'freq': 0},\n",
       " 'jar': {'id': 8004, 'freq': 34},\n",
       " 'lant': {'id': 21527, 'freq': 5},\n",
       " 'pare': {'id': 19885, 'freq': 3},\n",
       " 'contraceptives': {'id': 21365, 'freq': 6},\n",
       " 'situation': {'id': 929, 'freq': 2427},\n",
       " 'defeated': {'id': 16365, 'freq': 10},\n",
       " 'warriors': {'id': 18480, 'freq': 8},\n",
       " 'ferry': {'id': 15733, 'freq': 11},\n",
       " 'richter': {'id': 19158, 'freq': 7},\n",
       " '##affordable': {'id': 18523, 'freq': 0},\n",
       " 'expier': {'id': 16024, 'freq': 18},\n",
       " 'negot': {'id': 4316, 'freq': 2},\n",
       " '##aned': {'id': 14619, 'freq': 8},\n",
       " 'executive': {'id': 6493, 'freq': 76},\n",
       " 'avo': {'id': 8104, 'freq': 6},\n",
       " 'crime': {'id': 1625, 'freq': 964},\n",
       " 'popped': {'id': 20525, 'freq': 6},\n",
       " '1991': {'id': 12363, 'freq': 18},\n",
       " 'housekeeper': {'id': 8416, 'freq': 45},\n",
       " '##ella': {'id': 14940, 'freq': 18},\n",
       " 'certific': {'id': 6999, 'freq': 1},\n",
       " '##arning': {'id': 16798, 'freq': 5},\n",
       " 'waht': {'id': 10507, 'freq': 26},\n",
       " 'sleeps': {'id': 18087, 'freq': 8},\n",
       " 'grilled': {'id': 18944, 'freq': 7},\n",
       " 'winters': {'id': 19786, 'freq': 7},\n",
       " 'sports': {'id': 2043, 'freq': 629},\n",
       " '##ainted': {'id': 8270, 'freq': 3},\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47f8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame.from_dict(freq_dist, orient ='index')\n",
    "freq = freq.sort_values(by='freq', ascending=False)\n",
    "# trim out sparse tokens\n",
    "freq = freq.query(\"`freq` >= 24\")\n",
    "freq['log_freq'] = freq['freq'].apply(lambda x: np.log(x))\n",
    "freq = freq.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7bd9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(freq['log_freq'], bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df00fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the threshold up even farther - sparse tokens are \n",
    "# also more likely to be coincidentally biased\n",
    "freq = freq.query(\"`log_freq` >= 4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ccdd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx_map = {b:a for a,b in enumerate(freq['index'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb76f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_freq_vecs(dataframe, token_idx_map, frequency_dist, tfidf=False):\n",
    "    target_matrix = np.array([target_idx[a] for a in dataframe['Target']])\n",
    "    feature_matrix = np.zeros((len(dataframe),len(frequency_dist)))\n",
    "    for s, row in enumerate(dataframe['Text']):\n",
    "        tokens = tokenizer.tokenize(row)\n",
    "        for tok in tokens:\n",
    "            try:\n",
    "                ind = token_idx_map[tok]\n",
    "                feature_matrix[s,ind] += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if tfidf:\n",
    "        idf_smooth, feature_matrix = tf_idf(feature_matrix) \n",
    "        return idf_smooth, feature_matrix, target_matrix\n",
    "    else:\n",
    "        return feature_matrix, target_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9364ea8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make an identical dataframe with frequencies of arrays\n",
    "def get_doc_freq_vecs(dataframe, token_idx_map, frequency_dist, tfidf=False):\n",
    "    target_matrix = np.array([target_idx[a] for a in dataframe['Target']])\n",
    "    feature_matrix = np.zeros((len(dataframe),len(frequency_dist)))\n",
    "    \n",
    "    new_df = dataframe.copy()\n",
    "    new_df['Target_ID'] = np.array([target_idx[a] for a in dataframe['Target']])\n",
    "    for s, row in enumerate(dataframe['Text']):\n",
    "        tokens = tokenizer.tokenize(row)\n",
    "        for tok in tokens:\n",
    "            try:\n",
    "                ind = token_idx_map[tok]\n",
    "                feature_matrix[s,ind] += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    #new_df['Token_Freq'] = feature_matrix\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f156e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_data =get_doc_freq_vecs(full_dataset, token_idx_map, freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98dde90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency_Data\n",
    "target_inds = np.array([target_idx[a] for a in full_dataset['Target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acfec4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2892"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frequency_data[np.where(target_inds == 1),:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff0507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = full_dataset['k'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "174afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Target'] = np.array([target_idx[a] for a in full_dataset['Target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03653aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these results to data_dir\n",
    "token_idx = json.dumps(token_idx_map)\n",
    "with open('token_idx_map.json', 'w') as file:\n",
    "    file.write(token_idx)\n",
    "\n",
    "with open('frequency_data.npy', 'wb') as file:\n",
    "    np.save(file, frequency_data)\n",
    "\n",
    "with open('target_inds.npy', 'wb') as file:\n",
    "    np.save(file, target_inds)\n",
    "\n",
    "with open('k.npy', 'wb') as file:\n",
    "    np.save(file, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29e67e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('freq_feature_matrix.npy', 'wb') as file:\n",
    "    np.save(file, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab0b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(target_matrix).to_csv('target_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_matrix).to_csv('feature_relative_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('bayesian_linear_classifier')\n",
    "with open('feature_counts_matrix.npy', 'wb') as file:\n",
    "    np.save(file, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18432a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('bayesian_linear_classifier')\n",
    "with open('feature_percent_matrix.npy', 'wb') as file:\n",
    "    np.save(file, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab63eee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/paulp/Library/CloudStorage/OneDrive-UniversityofEasternFinland/UEF/Thesis/Data\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7714c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BertTokenizerFast in module transformers.models.bert.tokenization_bert_fast object:\n",
      "\n",
      "class BertTokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
      " |  BertTokenizerFast(vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |  \n",
      " |  Construct a \"fast\" BERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
      " |  \n",
      " |  This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
      " |  refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (`str`):\n",
      " |          File containing the vocabulary.\n",
      " |      do_lower_case (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not to lowercase the input when tokenizing.\n",
      " |      unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
      " |          The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      " |          sequence classification or for a text and a question for question answering. It is also used as the last\n",
      " |          token of a sequence built with special tokens.\n",
      " |      pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
      " |          The token used for padding, for example when batching sequences of different lengths.\n",
      " |      cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
      " |          The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      " |          instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      " |      mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
      " |          The token used for masking values. This is the token used when training this model with masked language\n",
      " |          modeling. This is the token which the model will try to predict.\n",
      " |      clean_text (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
      " |          whitespaces by the classic one.\n",
      " |      tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n",
      " |          issue](https://github.com/huggingface/transformers/issues/328)).\n",
      " |      strip_accents (`bool`, *optional*):\n",
      " |          Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      " |          value for `lowercase` (as in the original BERT).\n",
      " |      wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n",
      " |          The prefix for subwords.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertTokenizerFast\n",
      " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None)\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens. A BERT sequence has the following format:\n",
      " |      \n",
      " |      - single sequence: `[CLS] X [SEP]`\n",
      " |      - pair of sequences: `[CLS] A [SEP] B [SEP]`\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`):\n",
      " |              List of IDs to which the special tokens will be added.\n",
      " |          token_ids_1 (`List[int]`, *optional*):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]\n",
      " |      Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
      " |      pair mask has the following format:\n",
      " |      \n",
      " |      ```\n",
      " |      0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " |      | first sequence    | second sequence |\n",
      " |      ```\n",
      " |      \n",
      " |      If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (`List[int]`, *optional*):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (`str`, *optional*):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'TurkuNLP/bert-base-finnish-cased-v1': 512, '...\n",
      " |  \n",
      " |  pretrained_init_configuration = {'TurkuNLP/bert-base-finnish-cased-v1'...\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'tokenizer_file': {'TurkuNLP/bert-base-f...\n",
      " |  \n",
      " |  slow_tokenizer_class = <class 'transformers.models.bert.tokenization_b...\n",
      " |      Construct a BERT tokenizer. Based on WordPiece.\n",
      " |      \n",
      " |      This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
      " |      this superclass for more information regarding those methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_file (`str`):\n",
      " |              File containing the vocabulary.\n",
      " |          do_lower_case (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to lowercase the input when tokenizing.\n",
      " |          do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to do basic tokenization before WordPiece.\n",
      " |          never_split (`Iterable`, *optional*):\n",
      " |              Collection of tokens which will never be split during tokenization. Only has an effect when\n",
      " |              `do_basic_tokenize=True`\n",
      " |          unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
      " |              The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |              token instead.\n",
      " |          sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
      " |              The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      " |              sequence classification or for a text and a question for question answering. It is also used as the last\n",
      " |              token of a sequence built with special tokens.\n",
      " |          pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
      " |              The token used for padding, for example when batching sequences of different lengths.\n",
      " |          cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
      " |              The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      " |              instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      " |          mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
      " |              The token used for masking values. This is the token used when training this model with masked language\n",
      " |              modeling. This is the token which the model will try to predict.\n",
      " |          tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to tokenize Chinese characters.\n",
      " |      \n",
      " |              This should likely be deactivated for Japanese (see this\n",
      " |              [issue](https://github.com/huggingface/transformers/issues/328)).\n",
      " |          strip_accents (`bool`, *optional*):\n",
      " |              Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      " |              value for `lowercase` (as in the original BERT).\n",
      " |  \n",
      " |  \n",
      " |  vocab_files_names = {'tokenizer_file': 'tokenizer.json', 'vocab_file':...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`int` or `List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or `List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int` or `List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
      " |      Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n",
      " |      often want to remove sub-word tokenization artifacts at the same time.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`List[str]`): The token to join in a string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The joined tokens.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_vocab(self) -> Dict[str, int]\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the\n",
      " |      vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\n",
      " |      this inside your training loop.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  set_truncation_and_padding(self, padding_strategy: transformers.utils.generic.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int])\n",
      " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
      " |      library) and restore the tokenizer settings afterwards.\n",
      " |      \n",
      " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
      " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
      " |      section.\n",
      " |      \n",
      " |      Args:\n",
      " |          padding_strategy ([`~utils.PaddingStrategy`]):\n",
      " |              The kind of padding that will be applied to the input\n",
      " |          truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):\n",
      " |              The kind of truncation that will be applied to the input\n",
      " |          max_length (`int`):\n",
      " |              The maximum size of a sequence.\n",
      " |          stride (`int`):\n",
      " |              The stride to use when handling overflow.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |  \n",
      " |  tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, replacing unknown tokens with the `unk_token`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          pair (`str`, *optional*):\n",
      " |              A second sequence to be encoded with the first.\n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to add the special tokens associated with the corresponding model.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific encode method. See details in\n",
      " |              [`~PreTrainedTokenizerBase.__call__`]\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of tokens.\n",
      " |  \n",
      " |  train_new_from_iterator(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
      " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
      " |      as the current one.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_iterator (generator of `List[str]`):\n",
      " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
      " |              if you have everything in memory.\n",
      " |          vocab_size (`int`):\n",
      " |              The size of the vocabulary you want for your tokenizer.\n",
      " |          length (`int`, *optional*):\n",
      " |              The total number of sequences in the iterator. This is used to provide meaningful progress tracking\n",
      " |          new_special_tokens (list of `str` or `AddedToken`, *optional*):\n",
      " |              A list of new special tokens to add to the tokenizer you are training.\n",
      " |          special_tokens_map (`Dict[str, str]`, *optional*):\n",
      " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
      " |              token name to new special token name in this argument.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments passed along to the trainer from the 🤗 Tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on\n",
      " |          `text_iterator`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  backend_tokenizer\n",
      " |      `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
      " |  \n",
      " |  decoder\n",
      " |      `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  vocab_size\n",
      " |      `int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __annotations__ = {'can_save_slow_tokenizer': <class 'bool'>, 'slow_to...\n",
      " |  \n",
      " |  can_save_slow_tokenizer = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in `encode_plus`).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
      " |      `self.pad_token_id` and `self.pad_token_type_id`).\n",
      " |      \n",
      " |      Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
      " |      text followed by a call to the `pad` method to get a padded encoding.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |      result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
      " |      PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n",
      " |              tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n",
      " |              List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
      " |              collate function.\n",
      " |      \n",
      " |              Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
      " |              the note above for the return type.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Optional[List[int]] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*\n",
      " |      different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return\n",
      " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Optional[List[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (`list`, *optional*):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to `None`, this will use the predefined model maximum length if a maximum length is\n",
      " |              required by one of the truncation/padding parameters. If the model has no specific maximum input length\n",
      " |              (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (`int`, *optional*):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to `None`, this will use the max_length value.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to `self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, use_auth_token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '10GB', create_pr: bool = False, **deprecated_kwargs) -> str\n",
      " |      Upload the tokenizer files to the 🤗 Model Hub while synchronizing a local clone of the repo in\n",
      " |      `repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your tokenizer to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload tokenizer\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`).\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoTokenizer\n",
      " |      \n",
      " |      tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |      # Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the tokenizer to an organization with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..\n",
      " |      \n",
      " |      Warning,None This won't save modifications you may have applied to the tokenizer after the instantiation (for\n",
      " |      instance, modifying `tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (`bool`, *optional*):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
      " |              added_tokens files.\n",
      " |      \n",
      " |              If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with\n",
      " |              \"slow\" tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be\n",
      " |              loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If `True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a value\n",
      " |              error is raised.\n",
      " |          filename_prefix: (`str`, *optional*):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs:\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of `str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Optional[List[int]] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |          num_tokens_to_remove (`int`, *optional*, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will truncate\n",
      " |                token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a\n",
      " |                batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n",
      " |                than the model maximum admissible input size).\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple[List[int], List[int], List[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of\n",
      " |          overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair\n",
      " |          of sequences (or a batch of pairs) is provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
      " |      tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      " |                user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      " |              - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
      " |                `./my_model_directory/`.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                `./my_model_directory/vocab.txt`.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__` method.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
      " |              `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Passing `use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
      " |      # Download vocabulary from huggingface.co and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      " |      \n",
      " |      # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |      \n",
      " |      # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
      " |      \n",
      " |      # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
      " |      \n",
      " |      # You can link tokens to special vocabulary when instantiating\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", unk_token=\"<unk>\")\n",
      " |      # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |      # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |      assert tokenizer.unk_token == \"<unk>\"\n",
      " |      ```\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoTokenizer') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n",
      " |      library are already mapped with `AutoTokenizer`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):\n",
      " |              The auto class to register this new tokenizer with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      `int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      `int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'token_type_ids', 'attention_mask']\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  truncation_side = 'right'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      Note,None When adding new tokens to the vocabulary, you should make sure to also resize the token embedding\n",
      " |      matrix of the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n",
      " |      `'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
      " |              `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the `unk_token` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to add a new classification token to GPT-2\n",
      " |      tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      " |      model = GPT2Model.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |      assert tokenizer.cls_token == \"<CLS>\"\n",
      " |      ```\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n",
      " |      algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
      " |      not treated in the same way.\n",
      " |      \n",
      " |      Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
      " |      of the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
      " |              token to let you personalize its behavior: whether this token should only match against a single word,\n",
      " |              whether this token should strip all potential whitespaces on the left side, whether this token should\n",
      " |              strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |      tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
      " |      model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      ```\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (`tokenizer.mask_token`,\n",
      " |      `tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          `int`: The number of tokens added in the vocabulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      `List[str]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      `List[Union[str, tokenizers.AddedToken]]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      `int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      `Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (`cls_token`,\n",
      " |      `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      `Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping\n",
      " |      special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      `List[str]`: All the additional special tokens you may want to use. Log an error if used while not having been\n",
      " |      set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      `List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  bos_token\n",
      " |      `str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      `Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns `None` if the token has not\n",
      " |      been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      `str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the full\n",
      " |      depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      `Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input sequence\n",
      " |      leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      `str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      `Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      `str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      `str`: Separation token, to separate context and query in an input sequence. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      `Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  unk_token\n",
      " |      `str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      `Optional[int]`: Id of the unknown token in the vocabulary. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "488fc74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='custom_bert_tokenizer', vocab_size=22500, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<?>', '<*>', '<R>', '<MISC>', '<ORG>', '<LOC>', '<PER>']})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec1c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyto_env",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
